
# Lecture 2025-01-06
Random vectors in $\mathbb{R}^d, d \geq 2$.
$$
X=\left(\begin{array}{c}
x_1 \\
\vdots \\
x_0
\end{array}\right)
$$

$$
P\left(X \in \sum_{i=1}^j\left[a_i, b_i\right] .\right)
$$

$$
\bar{X}_1 \in\left[a_1, b_1\right], x_2 \in\left[a_2, b_2\right], \ldots, x_d \in\left[a_d, b_d\right]
$$

Examples
Binomial 2 outcomes.
this how more than 2.
(1) $Z \sim \operatorname{multinomial}\left(n ; p_1, \ldots, p_d\right)$ where $\sum_{i=1}^j p_i=1 p_1 \geq 0$
$d$ outcomes: $O_1, \ldots, O_j$

$X = \left(X_i\right)_{i \leq i \leq d} X_i=$ # of  outcomes $i \quad 1 \leq i \leq d$.

$$
X_1 \in(0,1, \ldots, n)
$$

$X=\sum_{i=1}^n Y_i, \quad\left\{Y_i\right\}_{1 \leq k \leq d}$ Ind.

$$
Y_k \in\left\{\left(\begin{array}{l}
0 \\
\vdots \\
0 \\
1 \\
0 \\
\vdots \\
0
\end{array}\right)-X_i\right\} \text { if } i \text { occured in } k \text { experiment. }
$$

$\therefore x_1$ is ...
$x_1 \sim$ Binomial $\left(n, p_1\right)$

$$
P\left(Z=\left(\begin{array}{c}
n_1 \\
\vdots \\
n_0
\end{array}\right)\right)=\frac{n!}{\prod_{k=1}^n (n k)!} \prod_{k=1}^{d} P_i^{n_i}\left\{\begin{array}{l}
\text { CDF } \\
\text { of } \\
\text { Multinomial }
\end{array}\right. \\
 \sum_{c=1}^{d} n_i=n .

$$

People are interested in coverince matrix of $X$. have $d$ coordinates $d \times d$ matrix.
Covariance matrix $\Gamma(X)=\left[\Gamma_{i j}\right]_{1 \leqslant i, j \leqslant d}$.

$$
\begin{aligned}
\Gamma_{i, j} & =\operatorname{Cov}\left(X_i, X_j\right) \\
& =E\left(X_i, X_j\right)-E\left(X_i\right) E\left(X_j\right) \\
\Gamma_{i, i} & =V\left(X_i^{\prime}\right)=1 p_i\left(1-p_i\right) \quad 1 \leq i \leq \partial . \\
\Gamma_{i, j} & =E\left(X_i, X_j\right)-E\left(X_i\right) E\left(X_j\right)
\end{aligned}
$$
---
$$
\begin{aligned}
& X_i=\sum_{k=1}^n \varepsilon_k, \varepsilon_k \sim \operatorname{Ber}\left(P_i\right), \quad\left\{\varepsilon_k\right\}_{1 \leq k \leq n} \text { ind. } \\
& X_j=\sum_{l=1}^n \delta_l, \quad \delta_l \sim \operatorname{Ber}\left(P_j\right) \quad\left\{\delta_l\right\}_{1 \leq k s n \text { ind. }} \\
& \operatorname{Cov}\left(X_i, X_i\right)=\sum_{1 \leq k, l \leq n} \operatorname{Cov}\left(\varepsilon_k, \delta_l\right)=\sum_{k=1}^n \operatorname{Cov}\left(\varepsilon_k, \delta_k\right)
\end{aligned}
$$

**remark**. Either $\varepsilon_1 = 1$ and $d_1 =0$, $\varepsilon_1=0,  d_1 = 1$, $\therefore e_1d_1=0$.

$$
\begin{aligned}
=n \operatorname{cov}\left(\varepsilon_1 \delta_1\right)= & n\left[E\left(\varepsilon_1 \delta_1\right)-E\left(\varepsilon_1\right) E\left(\delta_1\right)\right] \\
& n\left[O-p_i p_j \quad\right. \text { cov } \\
\therefore \quad & \Gamma_{i, j}= - np_ip_j
\end{aligned}
**remark**. The coveriance is negative.
$$

$X \sim$ multivariate normal $(\mu, \Gamma) \quad \mu \in \mathbb{R}^d$

$\vec{\mathbb{E}(X)}=\vec{\mu}$

$$
E(x)=\left(\begin{array}{c}
\mathbb{E}x_n\\
\vdots \\
\mathbb{E}X_n
\end{array}\right)
$$

$\Gamma=\left[\operatorname{cov}\left(X_i, X_j\right)\right]_{1 \leqslant i, j \leqslant d}$


if $\vec{X}^\mathcal{D}=\vec{M}+A \vec{Z}$
Where $A \in M_{d \alpha d} \leftarrow$ matrix

$A=\left[a_{i j}\right]_{1 \leq i, j \leq d}$

$$
\vec{Z}=\left(\begin{array} { c } 
Z_1 \\
\vdots\\
Z_d
\end{array} \right)
$$


$$
\{Z_i\}_{1\le i\le d} \text{ ind } N(0,1)
$$

what happend to covariance matrix

$\vec{Y} \in \mathbb{R}^d, E(Y)=0$

$\Gamma(Y)=YY^T$

$\mathbb{R}^{\delta \times \delta}$

$ \Gamma(Y)=E\left(Y \cdot Y^{+}\right)$

$ \Gamma=E\left((x-\mu)(x-\mu)^T\right)$

$\qquad=A Z(A Z)^T$

**remark**. $(A B)^t=B^t A^t$

$=A Z Z^t A^t$

$A E \underbrace{\left(z z^t\right)}_I A^t$

$=A A^t$

$A A^t$ is symmetric matrix.

X is R.V. is Multinormal
iff $t \cdot X=\langle t, X\rangle$ is $N\left(\mu, \sigma^2\right) . \quad \forall \vec(t) \in \mathbb{R}^d$. $t X=\sum_{i=1}^n t_i x_i$

Multivariate and multinormalss are 2 important examples how to characterize the distribution.

---

Let $X \in \mathbb{R}^{+}$
$$
F_x\left(t_1, \ldots, t_d\right)=P\left(x_1 \leq t_1, \ldots, x_d \leq t_d\right), \quad Z \in \mathbb{R}^2
$$
you charaterize to the distribution with rectangles

The Important char is not that even through books says Disitribution of X is characterized by $\operatorname{Dis}{t\cdot X}_{t\in\mathbb{R}^d}.

Namely, $P(t \cdot X \leq u) \quad u \in \mathbb{R}$.
this looks like half plane ${tX\le u}$ in 2d.

if $\|t\|_2=1 \quad$ Norm $\quad \sum_{t=1}^2 t_i^2=1$
since for all $t$ we can consider $u$ part of $t$,
$$
\varphi_X(t)=\varphi_{t \cdot x}(1)=E e^{i(t \cdot X)}, t \in \mathbb{R}^d.
$$

Claim this is the characteristic function. of $x$.
$$
X \in \mathbb{R}^{d}, t \in \mathbb{R}^{d}
$$
inversion formala
Proved similiar to W.
no cher. Func in infinite Dim.
Problem use, $\varphi_x(t)$ to calculute $P(x \in A)$ where
$$
\begin{aligned}
& A=\sum_{i=1}^d\left(a_i, b_i\right) \\
& P\left(X \in J_A\right)^K \text { the boundry. }
\end{aligned}
$$
$x$ subtract from ind. uniform.
$$
\begin{array}{ll}
Y=X-U & U \sim \text { uniform }(A) \\
& f_U(t)=\frac{1}{\text { volue }(A)} \quad X \in A .
\end{array}
$$

Where $U \text{ and } x$ ind. 

if $x \notin A$ then $0$.

$Y$ is Bounded, intrgable, Even of $x$ is next,
$$
f_{\vec{y}}(\overrightarrow{0})=P(x \in A) .
$$

# Lecture 2025-01-15

Last time
inverson formula.
$$
X \in \mathbb{R}^d 
\varphi_{X}(t)=E e^{i(t-X)} 
t \in \mathbb{R}^d

t X=\sum_{i=1}^d t_i X_i
$$

$$
A=\prod_{i=1}^A\left[a_i b_i\right]
$$
Invession formla 
move from $U$ to $T$ because $\varphi_U(t)=E e^{i t U}$.
$$
P(x \in A)=\lim _{T \rightarrow \infty}(2 \pi)^{-\alpha} \lambda(A) \cdot \int_{[-T, T]^d} \varphi_U(-t) \varphi_X(t) dt
$$
Lebesque Measure Aka Volume

Important trick.
$$
Y=X-U, \quad U \sim \text { uniform }(A), \quad(X, U) \text { InD. } \\
\varphi_Y(t)=E\left(e^{i t \cdot(X-U)}\right)=E\left(e^{i t x} \cdot e^{i(-t) U}\right) \\
f_Y(0)=\frac{P(x \in A)}{\lambda(A)}=\varphi_x(t) \cdot \varphi_U(-t) .

$$

Let $Y$ be Random vector. with "nice" density and $\varphi_Y(t)$ is the **ch.f**.
then $\lim _{T \rightarrow \infty}(2 \pi)^{-\delta} \int_{[-T, T]^d} \varphi_Y(t) dt =f_Y(0)$

Where [-T, T]^d represents an expanding hyper cube.

If we use fubini mass accummulates at 0.
$$
f_U(v)=\frac{1}{\lambda(A)}, \quad v \in A \\
v=\left(\begin{array}{c}
u_1 \\
\vdots \\
u_d
\end{array}\right) \\
U_k \sim \text { uniform }\left(a_k, b_k\right) \\
\perp\!\!\!\perp\left(U_k\right)_{1 \leq k \leq 0}

$$