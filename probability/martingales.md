
---
title: Martingales
permalink: /probability/martingales/
layout: default
---

# Martingales

## Filtrations

Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space.

A **filtration** $(\mathcal F_n)_{n \ge 0}$ is a non-decreasing sequence of $\sigma$-algebras:
$$
\mathcal F_0 \subseteq \mathcal F_1 \subseteq \cdots \subseteq \mathcal F.
$$

Interpretation: $\mathcal F_n$ represents all information available up to time $n$.



**Canonical example (discrete time):**
- $\Omega = \{0,1\}^{\mathbb N}$
- $\mathcal F_n$ generated by the first $n$ coordinates
- First $n$ coordinates fixed, rest free

---

## Stopping Times

A random variable $\tau : \Omega \to \mathbb N \cup \{\infty\}$ is a **stopping time** w.r.t. $(\mathcal F_n)$ if
$$
\{\tau \le n\} \in \mathcal F_n \quad \text{for all } n.
$$

Equivalent (discrete time):
$$
\{\tau = n\} \in \mathcal F_n \quad \forall n.
$$

### Stopping Time Properties
For **stopping times** $S$ and $T$, and constant $c$ the following is always true,
- $min(S,T)$ and  $max(S,T)$ is a **stopping time**.
- $c$ is a **stopping time** therefore $min(S,c)$ and $max(S,c)$ are a **stopping times**.
- S + c is a stopping time.
- 

### Not a Stopping Time
- S+T in general (see composite stopping times.)


### Examples
- Constants: $\tau \equiv c$
- First hitting time: $\tau = \inf\{n : X_n \in A\}$

### Closure Properties
If $\tau, \sigma$ are stopping times, then:
- $\min(\tau,\sigma)$, $\max(\tau,\sigma)$ are stopping times
- $\tau + c$ (constant $c$) is a stopping time

### Composite Stopping Times
If $\tau$ is a stopping time w.r.t. $(\mathcal F_n)$, and $\sigma$ is a stopping time w.r.t. the shifted filtration $(\mathcal F_{\tau+n})$, then
$$
\tau + \sigma
$$
is a stopping time w.r.t. $(\mathcal F_n)$.

(Justified via Strong Markov Property in BM settings.)

---

## Martingales

Let $(X_n, \mathcal F_n)$ be a stochastic process.

### Definition
$(X_n)$ is a **martingale** if:
1. **Adapted:** $X_n$ is $\mathcal F_n$-measurable
2. **Integrable:** $\mathbb E\|X_n\| < \infty,  \forall n=1,2,...$
3. **Fair game:**  
$$
\mathbb E[X_{n+1} \mid \mathcal F_n] = X_n
$$

### Variants
- **Submartingale:**  
$$
\mathbb E[X_{n+1} \mid \mathcal F_n] \ge X_n
$$
- **Supermartingale:**  
$$
\mathbb E[X_{n+1} \mid \mathcal F_n] \le X_n
$$

---
## **Martingale via increments (discrete time). 
Let $(\mathcal F_n)$ be a filtration and $(Y_n)$ be adapted with $\mathbb E|Y_n|<\infty$ for all $n$.
If the increments $\Delta Y_k := Y_k-Y_{k-1}$ satisfy
$$
\mathbb E(\Delta Y_k \mid \mathcal F_{k-1})=0 \quad \text{a.s. for all } k\ge 1,
$$
then $(Y_n,\mathcal F_n)$ is a martingale, since
$$
\mathbb E(Y_k\mid \mathcal F_{k-1})
=\mathbb E(Y_{k-1}+\Delta Y_k\mid \mathcal F_{k-1})
=Y_{k-1}.
$$

---

## Basic Properties

- Constant process is a martingale
- If $X_n$ is a martingale, then so is $\mathbb E[X_n \mid \mathcal G_n]$ for sub-filtration $\mathcal G_n \subseteq \mathcal F_n$
- Linear combinations of martingales are martingales

## Martingale Algebra (Variance, Covariance, Equality)

- Var(M_n) = sum of squared increments
- Cov(M_s, M_t) = Var(M_s)
- Orthogonality of increments
- Equality requires a.s. or L^2

---

## Stopped Martingales

If $(X_n)$ is a martingale and $\tau$ is a stopping time, define
$$
X_n^\tau := X_{n \wedge \tau}.
$$

**Theorem:**  
$(X_n^\tau)$ is a martingale w.r.t. $(\mathcal F_n)$.


---
## Shift Operators (Time Shifts)

Shift operators formalize the idea of **restarting a process after a stopping time**.
They are essential for:
- composite stopping times
- multi-stage hitting times
- gambling systems that restart
- Strong Markov–type arguments

---

### Definition (Pathwise Shift)

Let $\omega = (\omega_1, \omega_2, \ldots)$ be a sample path.

The **shift operator** $\theta_n$ acts on paths by
$$
(\theta_n \omega)_k := \omega_{n+k}, \quad k \ge 1.
$$

Interpretation:
- $\theta_n$ discards the first $n$ steps
- time is “reset” to zero after step $n$

---

### Shifted Process

Given a stochastic process $(X_n)$, define the shifted process
$$
(X \circ \theta_n)_k := X_{n+k}.
$$

This represents the process **as seen after time $n$**.

---

### Random Shifts (Stopping Times)

If $\tau$ is a stopping time, define the random shift
$$
(\theta_\tau \omega)_k := \omega_{\tau(\omega)+k}.
$$

Then:
$$
(X \circ \theta_\tau)_k = X_{\tau+k}.
$$

This is how we formalize “restart the process at time $\tau$”.

---

### Shifted Filtration

Given a filtration $(\mathcal F_n)$ and a stopping time $\tau$, define the **shifted filtration**
$$
\mathcal F^{(\tau)}_n := \mathcal F_{\tau+n}.
$$

A random time $\sigma$ is a stopping time *after* $\tau$ if
$$
\{\sigma \le n\} \in \mathcal F_{\tau+n}.
$$

---

### Composite Stopping Times

If:
- $\tau$ is a stopping time w.r.t. $(\mathcal F_n)$
- $\sigma$ is a stopping time w.r.t. $(\mathcal F^{(\tau)}_n)$

then:
$$
\tau + \sigma
$$
is a stopping time w.r.t. $(\mathcal F_n)$.

This is the correct way to form finite sums of stopping times.

---

### Martingales and Shift Operators

If $(M_n)$ is a martingale, then the shifted process
$$
(M_{\tau+n} - M_\tau)_{n \ge 0}
$$
is a martingale w.r.t. the shifted filtration $(\mathcal F_{\tau+n})$.

(This is the discrete-time version of the Strong Markov Property.)

---

### How Shift Operators Appear on Prelims

Look for phrases like:
- “after time $\tau$”
- “restart the process”
- “wait until the next event”
- “define another stopping time after hitting $a$”

These are signals that you need:
$$
X_{\tau+n} = X \circ \theta_\tau
\quad \text{and} \quad
\mathcal F_{\tau+n}.
$$

---

### Exam Heuristic

If you see **two stages in time**, think:
$$
\text{shift operator} \;\Rightarrow\; \text{shifted filtration} \;\Rightarrow\; \tau + \sigma.
$$

---
## Predictable Processes

A process $H_n$ is **predictable** w.r.t. $(\mathcal F_n)$ if
$$
H_n \text{ is } \mathcal F_{n-1}\text{-measurable for all } n \ge 1.
$$

If $X_n$ is a martingale and $H_n$ is bounded and predictable, then
$$
Y_n := \sum_{k=1}^n H_k (X_k - X_{k-1})
$$
is a martingale.

## Doob Decomposition

If $(X_n)$ is a submartingale, then there exists a unique decomposition
$$
X_n = M_n + A_n
$$
where:
- $M_n$ is a martingale
- $A_n$ is predictable, increasing, and $A_0 = 0$

Explicitly,
$$
A_n = \sum_{k=1}^n \mathbb E[X_k - X_{k-1} \mid \mathcal F_{k-1}].
$$

## Convex Transformations

If $(X_n)$ is a martingale and $\varphi$ is convex with $\mathbb E|\varphi(X_n)|<\infty$, then
$$
\varphi(X_n) \text{ is a submartingale}.
$$

If $\varphi$ is increasing and convex:
- subMG → subMG
- superMG → superMG

## Orthogonality of Martingale Increments

Let $(M_n)$ be a martingale in $L^2$.

If $i < j$, then
$$
\mathbb E[(M_j - M_{j-1})(M_i - M_{i-1})] = 0.
$$

## Conditional Variance Formula

If $(M_n)$ is a martingale in $L^2$, then
$$
\mathrm{Var}(M_n)
= \sum_{k=1}^n \mathbb E\left[\mathrm{Var}(M_k \mid \mathcal F_{k-1})\right].
$$

---

## Martingale Convergence Theorem (MGCT)

Let $(X_n)$ be a submartingale.

### Version 1 (Boundedness)
If $(X_n)$ is bounded in $L^1$ and either:
- bounded above (subMG), or
- bounded below (superMG),

then
$$
X_n \to X_\infty \quad \text{a.s.}
$$

### Version 2 (Uniform Integrability)
If $(X_n)$ is a **uniformly integrable martingale**, then
$$
X_n \to X_\infty \quad \text{a.s. and in } L^1.
$$

---

## Uniform Integrability (UI)

A family $\{X_n\}$ is **uniformly integrable** if
$$
\lim_{K \to \infty} \sup_n \mathbb E\left[|X_n| \mathbf 1_{\{|X_n|>K\}}\right] = 0.
$$

### Equivalent Conditions (TFAE)
For integrable $X_n$:
1. $(X_n)$ is UI
2. $\sup_n \mathbb E\|X_n\| < \infty$ and tails vanish
3. $X_n \to X$ in $L^1$ implies UI

### Useful Facts
- $L^p$-bounded ($p>1$) ⇒ UI
- Bounded martingale ⇒ UI
- UI preserved under stopping

---

## Doob’s Inequalities

### Doob Maximal Inequality (Submartingale)
If $(X_n)$ is a nonnegative submartingale, then for $\lambda > 0$,
$$
\mathbb P\left(\max_{k \le n} X_k \ge \lambda\right)
\le \frac{\mathbb E[X_n]}{\lambda}.
$$

### $L^p$ Inequality
If $X_n$ is a martingale and $p>1$,
$$
\mathbb E\left[\max_{k \le n} |X_k|^p\right]
\le \left(\frac{p}{p-1}\right)^p \mathbb E|X_n|^p.
$$

Consequences:
- $L^p$-bounded MG ⇒ UI
- Almost sure convergence

## Upcrossing Inequality

Let $(X_n)$ be a submartingale and $a < b$.
Let $U_n(a,b)$ be the number of upcrossings of $[a,b]$ by time $n$.

Then
$$
(b-a)\mathbb E[U_n(a,b)] \le \mathbb E[(X_n - a)^+].
$$

Consequence:
Finite upcrossings ⇒ almost sure convergence.

## Quadratic Variation (Discrete Time)

For a martingale $(M_n)$,
$$
[M]_n := \sum_{k=1}^n (M_k - M_{k-1})^2.
$$

Then:
- $M_n^2 - [M]_n$ is a martingale
- Used for variance tracking and CLTs

## Maximal Inequality (Supermartingale)

If $(X_n)$ is a nonnegative supermartingale, then
$$
\lambda \mathbb P\left(\sup_n X_n \ge \lambda\right)
\le \mathbb E[X_0].
$$

## Martingale Convergence Summary

| Assumption | Conclusion |
|----------|-----------|
| UI martingale | a.s. + $L^1$ convergence |
| $L^p$-bounded, $p>1$ | UI ⇒ convergence |
| Bounded MG | UI ⇒ convergence |
| SubMG bounded above | a.s. convergence |

---

## Optional Sampling Theorem (OST)

Let $(X_n)$ be a martingale and $\sigma \le \tau$ stopping times.

If **any one** of the following holds:
1. $\sigma,\tau$ bounded a.s.
2. $(X_n)$ is UI
3. Bounded increments + integrable stopping times

Then
$$
\mathbb E[X_\tau \mid \mathcal F_\sigma] = X_\sigma,
\quad
\mathbb E[X_\tau] = \mathbb E[X_\sigma].
$$

### OST Tricks
- Compute expectations at hitting times
- Prove fairness of gambling systems
- Reduce problems to stopped martingales

## Optional Stopping: Common Failure Modes

OST may fail if:
- Stopping time is unbounded
- Martingale is not UI
- Expected stopping time is infinite
- Martingale has heavy tails

Classic counterexample:
- Simple symmetric random walk
- Hitting time of a single point

---

## Martingale Difference Sequences (MD)

A sequence $(D_n)$ is a **martingale difference** if:
$$
\mathbb E[D_{n+1} \mid \mathcal F_n] = 0,
\quad
X_n := \sum_{k=1}^n D_k \text{ is a martingale}.
$$

### CLT for MD Arrays (Sketch)
If:
1. Conditional variances converge
2. Lindeberg-type condition holds

Then partial sums satisfy a CLT.

---

## Brownian Motion and Martingales

Standard Brownian Motion $(B_t)$:
- Continuous paths
- Independent increments
- $B_t - B_s \sim N(0, t-s)$

### Common Martingales
- $B_t$
- $B_t^2 - t$
- Exponentials via Girsanov

### Strong Markov Property
If $\tau$ is a stopping time, then
$$
(B_{\tau+t} - B_\tau)_{t \ge 0}
$$
is independent of $\mathcal F_\tau$ and is standard Brownian motion.

Used to justify:
- Composite stopping times
- Multi-stage hitting times

---

## Key Takeaways

- Always check **adapted + integrable + conditional expectation**
- Stopped martingales preserve structure
- UI is the cleanest path to convergence and OST
- Doob inequalities are your main technical engine
- You can always **create your own stopping time**

## Exam Heuristics

- See conditional expectation = current value → martingale
- See stopping time → try OST
- See maximum → Doob inequality
- See convergence → check UI or boundedness
- See gambling system → predictable process
- See Brownian hitting time → Strong Markov


## Gambling Systems (Martingale Betting Strategies)

Gambling systems formalize adaptive betting strategies applied to martingales and are best viewed as **predictable transformations of martingales**.

They encode the principle that **no fair game can be turned into a winning one using only past information**.

---

### Setup

Let $(X_n, \mathcal F_n)$ be a martingale.

Interpretation:
- $X_n$: wealth after round $n$
- $X_n - X_{n-1}$: payoff of round $n$

---

### Predictable Betting Strategy

A process $(H_n)$ is a **gambling system** if
$$
H_n \text{ is } \mathcal F_{n-1}\text{-measurable for all } n \ge 1.
$$

Interpretation:
- $H_n$ is the amount wagered *before* round $n$
- Strategy depends only on past information

Such processes are also called **predictable processes**.

---

### Gains Process

Given a gambling system $(H_n)$, define the **cumulative gains**
$$
Y_n := \sum_{k=1}^n H_k (X_k - X_{k-1}), \quad Y_0 = 0.
$$

- $Y_n$ represents the wealth generated purely by betting
- Often written as a discrete stochastic integral

---

### Fair Gambling Theorem

If:
- $(X_n)$ is a martingale
- $(H_n)$ is predictable and bounded

then
$$
(Y_n) \text{ is a martingale.}
$$

**Proof sketch:**
$$
\mathbb E[Y_{n+1} - Y_n \mid \mathcal F_n]
= H_{n+1} \mathbb E[X_{n+1} - X_n \mid \mathcal F_n]
= 0.
$$

**Interpretation:**  
No bounded betting strategy can create expected profit from a fair game.

---

### Submartingale and Supermartingale Cases

- If $X_n$ is a **submartingale** and $H_n \ge 0$, then $Y_n$ is a submartingale
- If $X_n$ is a **supermartingale** and $H_n \ge 0$, then $Y_n$ is a supermartingale

---

### Stopped Gambling Systems

Let $\tau$ be a stopping time.

Define the stopped gains:
$$
Y_n^\tau := \sum_{k=1}^{n \wedge \tau} H_k (X_k - X_{k-1}).
$$

Then:
- $Y_n^\tau$ is a martingale
- Allows strategies such as “stop when rich” or “stop when ruined”

---

### Optional Sampling for Gambling Systems

If:
- $(Y_n)$ is uniformly integrable, or
- $\tau$ is bounded a.s.

then
$$
\mathbb E[Y_\tau] = 0.
$$

**Interpretation:**  
Even with adaptive betting and stopping rules, expected profit remains zero.

---

### Failure Modes (Important for Prelims)

Optional Sampling may fail if:
- The betting strategy $(H_n)$ is unbounded
- The stopping time $\tau$ is unbounded
- Expected values are infinite

**Classic counterexample (Doubling Strategy):**
- Bet $H_n = 2^n$ after consecutive losses
- Predictable but unbounded
- Expected wealth undefined
- OST does not apply

---

### Exam Heuristics (Gambling Language)

- “Bet”, “strategy”, “capital”, “winnings” → gambling system
- “Depends only on the past” → predictability
- “Stop when…” → stopping time
- “Expected gain” → martingale + OST
- “Doubling” → unbounded strategy, OST failure
