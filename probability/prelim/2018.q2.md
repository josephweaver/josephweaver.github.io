---
title: "2018 Problem 2 — Gaussian Maxima, Extreme Value Limits, and Gumbel Convergence"
layout: default
math: true
---

# **Probability Prelim — August 2018**  
**Problem 2**  
:contentReference[oaicite:1]{index=1}

> Let $Z, Z_1, Z_2, \dots$ be iid random variables where $Z \sim N(0,1)$ and denote by  
> $f_Z(x)$, $-\infty<x<\infty$ the density of $Z$.  
> Also let  
> $$
> M_n = \max_{1\le k\le n} Z_k.
> $$
>
> **(a)** Recall the inequality, for $x \ge 0$,
> $$
> \left(\frac1x - \frac1{x^3}\right) f_Z(x)
> \le P(Z>x) \le \frac1x\, f_Z(x).
> $$
>
> **(i)** Prove that  
> $$
> P(Z>x) \sim \frac{1}{x} f_Z(x),
> \qquad\text{namely}\qquad
> \frac{P(Z>x)}{(1/x) f_Z(x)} \xrightarrow[x\to\infty]{} 1.
> $$
>
> **(ii)** Prove that for each $y\in\mathbb{R}$,
> $$
> \frac{P\!\left(Z>x + \frac{y}{x}\right)}
> {P(Z>x)} \xrightarrow[x\to\infty]{} e^{-y}.
> $$
> *Hint: Use part (i).*
>
> ---
>
> **(b)** Let $a_n$ satisfy $P(Z > a_n)=1/n$ for $n\ge1$.  
> Let  
> $$
> q_n(y) = P\!\left(Z > a_n + \frac{y}{a_n}\right).
> $$
>
> **(i)** Prove that for each $y\in\mathbb{R}$,
> $$
> (1 - q_n(y))^n \xrightarrow[n\to\infty]{} e^{-e^{-y}}.
> $$
> *Hint: First show $n q_n(y) \to e^{-y}$.*
>
> **(ii)** Prove that  
> $$
> a_n(M_n - a_n) \xRightarrow[n\to\infty]{} Y,
> \qquad
> F_Y(y)=e^{-e^{-y}}.
> $$
>
> ---
>
> **(c)** Prove that  
> $$
> M_n - a_n \xrightarrow{P} 0
> \qquad\text{and conclude}\qquad
> \frac{M_n}{a_n} \xrightarrow{P} 1.
> $$

---

# **(a)(i) Asymptotics of the Gaussian Tail**

## **Assumptions / Notation**
- $Z\sim N(0,1)$ with density $\phi(x)=f_Z(x)$.
- Given inequalities:
  $$
  \left(\frac1x - \frac1{x^3}\right)\phi(x) \le P(Z>x) \le \frac1x\phi(x).
  $$

---

## **Solution**

### **Claim.**  
$$
\frac{P(Z>x)}{(1/x)\phi(x)} \xrightarrow[x\to\infty]{} 1.
$$

### **Proof.**

**Step 1. Divide the inequality by $\phi(x)$.**  
Since $\phi(x)>0$,
$$
\frac1x - \frac1{x^3}
\;\le\;
\frac{P(Z>x)}{\phi(x)}
\;\le\;
\frac1x.
$$

**Step 2. Multiply through by $x$.**
$$
1 - \frac1{x^2}
\;\le\;
\frac{P(Z>x)}{(1/x)\phi(x)}
\;\le\;
1.
$$

**Step 3. Take limits.**  
As $x\to\infty$,  
$$
1 - \frac1{x^2} \to 1.
$$

By the squeeze theorem:
$$
\frac{P(Z>x)}{(1/x)\phi(x)} \to 1.
$$

### **Conclusion.**  
$$
P(Z>x) \sim \frac{1}{x}\phi(x).
$$

---

## **Key Takeaways**
- This is a direct application of the squeeze theorem.  
- Gaussian tails are asymptotically equivalent to $(1/x)\phi(x)$.  
- This asymptotic expansion is the foundation of all extreme-value theory for Gaussians.

---

# **(a)(ii) Ratio of Shifted Tails**

## **Assumptions**
- Use result of (a)(i):
  $$
  P(Z>x) \sim \frac{1}{x}\phi(x).
  $$

---

## **Solution**

### **Claim.**  
For each $y\in\mathbb{R}$,
$$
\frac{P(Z>x+y/x)}{P(Z>x)} \to e^{-y}.
$$

### **Proof.**

**Step 1. Apply part (i) to numerator and denominator.**
$$
\frac{P(Z>x+y/x)}{P(Z>x)}
\sim
\frac{(1/(x+y/x))\phi(x+y/x)}{(1/x)\phi(x)}.
$$

**Step 2. Simplify the prefactor.**
$$
\frac{1/(x+y/x)}{1/x}
= \frac{1}{1 + y/x^2}
\xrightarrow[x\to\infty]{} 1.
$$

**Step 3. Compute the ratio of densities.**
$$
\frac{\phi(x+y/x)}{\phi(x)}
= \exp\!\left(
-\tfrac12(x+y/x)^2 + \tfrac12 x^2
\right)
= \exp\!\left(
-y - \frac{y^2}{2x^2}
\right)
\to e^{-y}.
$$

### **Conclusion.**  
$$
\frac{P(Z>x+y/x)}{P(Z>x)} \to e^{-y}.
$$

---

## **Key Takeaways**
- Ratios of Gaussian tails reduce to exponent algebra in the density.  
- The term $y/x^2$ vanishes, leaving $e^{-y}$.

---

# **(b)(i) Convergence of $(1 - q_n(y))^n$ to Gumbel Form**

## **Assumptions**
- $a_n$ defined by $P(Z>a_n)=1/n$.  
- $q_n(y)=P(Z>a_n+y/a_n)$.  
- From (a)(ii):
  $$
  \frac{q_n(y)}{1/n} \to e^{-y}.
  $$

---

## **Solution**

### **Claim.**  
$$
(1 - q_n(y))^n \to e^{-e^{-y}}.
$$

### **Proof.**

**Step 1. Use part (a)(ii) with $x=a_n$.**  
Since $P(Z>a_n)=1/n$,
$$
\frac{q_n(y)}{1/n}
=\frac{P(Z>a_n+y/a_n)}{P(Z>a_n)}
\to e^{-y}.
$$

Thus:
$$
n q_n(y) \to e^{-y}.
$$

**Step 2. Rewrite $(1 - q_n(y))^n$.**
$$
(1 - q_n(y))^n
= \left(1 - \frac{n q_n(y)}{n}\right)^n
\to e^{-\lim n q_n(y)}
= e^{-e^{-y}}.
$$

### **Conclusion.**  
$$
(1 - q_n(y))^n \to e^{-e^{-y}}.
$$

---

## **Key Takeaways**
- Classic limit:
  $$
  (1 - u_n)^n \to e^{-c} \quad \text{if } nu_n\to c.
  $$  
- This is the essential Gumbel limit mechanism.  
- $nq_n(y)$ represents the expected number of exceedances.

---

# **(b)(ii) Convergence of the Maxima to the Gumbel Distribution**

## **Assumptions**
- $M_n = \max Z_k$.  
- Independence implies  
  $$
  P(M_n \le z) = P(Z \le z)^n.
  $$  
- From (b)(i): $(1 - q_n(y))^n \to e^{-e^{-y}}$.

---

## **Solution**

### **Claim.**  
$$
a_n(M_n - a_n) \Rightarrow Y,
\qquad F_Y(y)=e^{-e^{-y}}.
$$

### **Proof.**

**Step 1. Rewrite the event.**
$$
P(a_n(M_n - a_n) \le y)
= P\left(M_n \le a_n + \frac{y}{a_n}\right).
$$

**Step 2. Use independence.**
$$
P(M_n \le z)
= P(Z\le z)^n
= (1 - P(Z>z))^n.
$$

Let $z = a_n + y/a_n$:
$$
P(a_n(M_n - a_n) \le y)
= (1 - q_n(y))^n.
$$

**Step 3. Apply (b)(i):**
$$
(1 - q_n(y))^n \to e^{-e^{-y}}.
$$

### **Conclusion.**  
$$
a_n(M_n - a_n) \Rightarrow Y,
\qquad P(Y\le y)=e^{-e^{-y}}.
$$  
This is the **Gumbel distribution**.

---

## **Key Takeaways**
- Maxima of Gaussians converge (after centering/scaling) to Gumbel.  
- Independence yields a product structure, reducing everything to tail approximations.

---

# **(c) Show $M_n - a_n \to 0$ in probability and $M_n/a_n \to 1$**

## **Assumptions**
- From (b)(ii):  
  $$
  X_n := a_n(M_n - a_n) \Rightarrow Y,
  $$
  where $Y$ is nondegenerate.  
- $a_n \to \infty$.

---

## **Solution**

### **Claim 1.**  
$$
M_n - a_n \xrightarrow{P} 0.
$$

### **Proof.**

**Step 1. Multiply and divide by $a_n$.**  
$$
M_n - a_n
= \frac{1}{a_n}\, a_n(M_n - a_n)
= \frac{1}{a_n} X_n.
$$

**Step 2. Apply Slutsky’s theorem.**  
- $X_n \Rightarrow Y$ (from part (b)(ii))  
- $1/a_n \to 0$

Thus:
$$
\frac{1}{a_n} X_n \Rightarrow 0.
$$

A convergence in distribution to a constant implies convergence in probability.

### **Conclusion.**  
$$
M_n - a_n \to 0 \quad \text{in probability}.
$$

---

### **Claim 2.**  
$$
\frac{M_n}{a_n} \xrightarrow{P} 1.
$$

### **Proof.**

For any $\varepsilon>0$,
$$
\left|\frac{M_n}{a_n} - 1\right| < \varepsilon
\quad \Leftrightarrow \quad
|M_n - a_n| < \varepsilon a_n.
$$

Since $a_n\to\infty$, for large $n$, $\varepsilon a_n$ is large, and  
$$
P(|M_n - a_n| < \varepsilon a_n)
\ge
P(|M_n - a_n| < \varepsilon)
\to 1.
$$

### **Conclusion.**  
$$
\frac{M_n}{a_n} \to 1 \quad \text{in probability}.
$$

---

## **Key Takeaways**
- Use the “multiply by 1” trick:  
  $$
  M_n - a_n = \frac{1}{a_n}\, a_n(M_n - a_n).
  $$
- Slutsky transforms extreme-value scaling into probability convergence.  
- Convergence in distribution to a constant implies convergence in probability.  
- Scaling arguments: multiplying $\varepsilon$ by $a_n\to\infty$ relaxes inequalities.

