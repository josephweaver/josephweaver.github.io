# 2021.Q2: Time Reversal and the Maximum of Brownian Motion

[2021 Probability Prelim Exam (PDF)](https://stt.natsci.msu.edu/_assets/files/graduate-programs/Prelim%20Prob%20Exam%202021.pdf)

## Introduction

This problem tests several core ideas about Brownian motion that often appear implicitly in prelim problems:

- Understanding **time-reversal of Brownian motion** and recognizing when it produces another Brownian motion in distribution.
- Working comfortably with **Gaussian processes**, especially using mean and covariance to identify distributions.
- Connecting **pathwise identities** (almost sure statements) with **distributional identities**, especially involving extrema.
- Correctly invoking the **reflection principle** and understanding what it actually gives.

**Main tools likely needed:**
- Covariance structure of Brownian motion.
- Characterization of Gaussian processes by mean and covariance.
- Continuity of Brownian sample paths.
- Reflection principle for Brownian motion.

**Pattern recognition cue:**  
If you see expressions like $B_{T-t} - B_T$, think “time reversal + centering,” and if you see maxima like $B_t^*$, expect the reflection principle to appear.

---

## Problem Statement (verbatim)

> **Problem 2.**  
> In this problem $\{B_t, t \ge 0\}$ represents a Standard Brownian Motion (SBM) defined on its canonical space  
$(\Omega = C[0, \infty), \{\mathcal{F}_t\}_{t \ge 0}, \mathcal{F}, P)$.
> 
> Let  
> $$
B_t^* = \max_{0 \le s \le t} \{B_s\}, \quad t \ge 0
> $$
> (“trailing maximum” of SBM). Observe that with this notation the reflection principle states:
> $$
B_t^* = |B_t| \quad \text{in distribution, for } t \ge 0.
> $$
> Let $T > 0$ be fixed.
> 
>  **a.** Let $Y_t = B_{T-t} - B_T,\; 0 \le t \le T.$  
> Calculate $\operatorname{Cov}(Y_t, Y_s),\; 0 \le t, s \le T.$
> 
> **b.** Prove that $\{Y_t, 0 \le t \le T\} = \{B_t, 0 \le t \le T\}$ in distribution.
> 
> **c.**  
> (i) Prove that $Y_T^* = B_T^* - B_T,$ a.s.  
>  (ii) Prove that $|B_T| = B_T^* - B_T$ in distribution.

---

## Solutions

### Part (a) Covariance computation

**Claim.**  
For $0 \le s,t \le T$,
$$
\operatorname{Cov}(Y_t, Y_s) = \min(s,t).
$$

**Proof.**  
By definition,
$$
Y_t = B_{T-t} - B_T, \qquad Y_s = B_{T-s} - B_T.
$$
Thus
$$
\operatorname{Cov}(Y_t,Y_s)
= \operatorname{Cov}(B_{T-t}-B_T,\; B_{T-s}-B_T).
$$
Expanding,
$$
\begin{aligned}
\operatorname{Cov}(Y_t,Y_s)
&= \operatorname{Cov}(B_{T-t},B_{T-s})
- \operatorname{Cov}(B_{T-t},B_T) \\
&\quad - \operatorname{Cov}(B_{T-s},B_T)
+ \operatorname{Cov}(B_T,B_T).
\end{aligned}
$$
Using $\operatorname{Cov}(B_a,B_b)=a\wedge b$,
$$
\begin{aligned}
\operatorname{Cov}(Y_t,Y_s)
&= (T-t)\wedge(T-s) - (T-t)\wedge T - (T-s)\wedge T + T.
\end{aligned}
$$
If $s \le t$, this simplifies to
$$
(T-t) - (T-t) - (T-s) + T = s.
$$
Similarly, if $t \le s$, the result is $t$. Hence
$$
\operatorname{Cov}(Y_t,Y_s) = \min(s,t).
$$

**Conclusion.**  
The covariance structure of $(Y_t)$ matches that of standard Brownian motion.

**Key Takeaways.**
- Brownian covariance: $\operatorname{Cov}(B_s,B_t)=s\wedge t$.
- Always expand covariance linearly before simplifying.
- Time-reversed increments often collapse cleanly after cancellation.

---

### Part (b) Equality in distribution of processes

**Claim.**  
$$
\{Y_t, 0 \le t \le T\} \stackrel{d}{=} \{B_t, 0 \le t \le T\}.
$$

**Proof.**  
First,
$$
E[Y_t] = E[B_{T-t} - B_T] = 0 - 0 = 0 = E[B_t].
$$
From part (a),
$$
\operatorname{Cov}(Y_s,Y_t) = s \wedge t = \operatorname{Cov}(B_s,B_t).
$$
The process $(Y_t)$ is Gaussian since it is a linear transformation of the Gaussian vector
$(B_{T-t}, B_T)$. A centered Gaussian process is completely determined by its covariance function.
Therefore, $(Y_t)$ and $(B_t)$ have the same finite-dimensional distributions, hence the same
distribution as processes on $[0,T]$.

**Conclusion.**  
$(Y_t)$ is a standard Brownian motion in distribution.

**Key Takeaways.**
- Linear transformations of Gaussian vectors remain Gaussian.
- For Gaussian processes, mean + covariance completely determine the law.
- “Equality in distribution of processes” means equality of all finite-dimensional distributions.

---

### Part (c)(i) Pathwise identity for the maximum

**Claim.**  
$$
Y_T^* = B_T^* - B_T \quad \text{a.s.}
$$

**Proof.**  
By definition,
$$
Y_T^* = \max_{0 \le s \le T} Y_s
= \max_{0 \le s \le T} (B_{T-s} - B_T).
$$
Since $B_T$ is constant in $s$,
$$
Y_T^* = \left(\max_{0 \le s \le T} B_{T-s}\right) - B_T.
$$
Let $u = T-s$. As $s$ runs from $0$ to $T$, $u$ runs from $T$ down to $0$, so
$$
\max_{0 \le s \le T} B_{T-s} = \max_{0 \le u \le T} B_u = B_T^*.
$$
Therefore,
$$
Y_T^* = B_T^* - B_T \quad \text{a.s.}
$$

**Conclusion.**  
The maximum of the time-reversed process has a simple pathwise relationship to the original maximum.

**Key Takeaways.**
- Maxima commute with time-reversal when paths are continuous.
- Be careful with change-of-variables in maxima: track bounds explicitly.
- This part is almost entirely deterministic once paths are fixed.

---

### Part (c)(ii) Distributional identity via reflection

**Claim.**  
$$
|B_T| \stackrel{d}{=} B_T^* - B_T.
$$

**Proof.**  
From part (b), $(Y_t)_{0 \le t \le T} \stackrel{d}{=} (B_t)_{0 \le t \le T}$.
Since Brownian motion has continuous sample paths, the maximum over $[0,T]$ is determined by the path,
and hence
$$
Y_T^* \stackrel{d}{=} B_T^*.
$$
From part (c)(i), we have the almost sure identity
$$
Y_T^* = B_T^* - B_T,
$$
so
$$
B_T^* - B_T \stackrel{d}{=} B_T^*.
$$
By the reflection principle,
$$
B_T^* \stackrel{d}{=} |B_T|.
$$
Combining these gives
$$
B_T^* - B_T \stackrel{d}{=} |B_T|.
$$

**Conclusion.**  
The reflected maximum minus the terminal value has the same law as the absolute value of Brownian motion.

**Key Takeaways.**
- Reflection principle: $B_T^* \stackrel{d}{=} |B_T|$.
- Continuity of paths allows extrema to inherit equality in distribution.
- Distinguish carefully between almost sure identities and distributional equalities.

---

## Master Key Takeaways

- Time-reversed Brownian motion (properly centered) is Brownian motion in distribution.
- For Gaussian processes, matching covariance and mean is enough.
- Almost sure identities can be combined with distributional identities to produce new laws.
- Continuity is the silent hypothesis that justifies passing from finite-dimensional distributions to maxima.

---

## Cheat Sheet Entries to Extract

- **Time-reversal trick:** $B_{T-t}-B_T$ has the same law as $B_t$ on $[0,T]$.
- **Gaussian process ID:** Mean $=0$ + covariance $= s\wedge t$ ⇒ standard Brownian motion.
- **Reflection principle:** $B_t^* \stackrel{d}{=} |B_t|$.
- **Maxima + continuity:** For continuous processes, equality in distribution passes to suprema.

---

## Notes on My Original Work

- Strong: Correct covariance computation and correct high-level strategy throughout.
- Needed patching: Clarifying the Gaussian-process argument in part (b).
- Main fix: Explicitly invoking continuity (rather than abstract “functionals”) when passing to maxima.
- Symbol errors in the handwritten version (e.g., $|B_T^*|$) were corrected for clarity.
