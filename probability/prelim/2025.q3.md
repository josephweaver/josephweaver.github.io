# 2025-Q3 – Characteristic Functions, Independence, and Poisson Limits in ℝ³

## Problem 3 (verbatim)

Let $(X,Y)\in\mathbb R^{2}$ be a random vector. The characteristic function (c.f.) of $(X,Y)$ is  
$$
\varphi_{(X,Y)}(s,t)=E(e^{i(sX+tY)}),\qquad (s,t)\in\mathbb R^2.
$$  
It is known that if the characteristic functions of $(X,Y)$ and $(U,V)$ are identical then they are equal in distribution.

---

### **(a)**  
Let $\varphi_X(s)$ and $\varphi_Y(t)$ be the c.f.’s of $X$ and $Y$.

(i) If $X,Y$ are independent, prove  
$$
\varphi_{(X,Y)}(s,t)=\varphi_X(s)\,\varphi_Y(t).
$$

(ii) If  
$$
\varphi_{(X,Y)}(s,t)=\varphi_X(s)\,\varphi_Y(t),
$$  
prove that $X,Y$ are independent.

---

### **(b)**  
Let $X=\sum_{m=1}^n X_m$ in $\mathbb R^3$, where the $X_m$ are i.i.d. and  
$$
P(X_1=\varepsilon_k)=p_k,\qquad k=1,2,3,
$$  
with  
$\varepsilon_1=(1,0,0)$,  
$\varepsilon_2=(0,1,0)$,  
$\varepsilon_3=(0,0,0)$, and $\sum p_k=1$.

(i) Find the c.f. of $X_1$, $\varphi_{X_1}(t)=E(e^{i t\cdot X_1})$, and determine how $X_{1,3}$ is distributed.

(ii) Find the c.f. of $X$, $\varphi_X(t)$.

---

### **(c)**  
Let $X_n=\sum_{m=1}^n X_{n,m}$ where $X_{n,m}$ are i.i.d. in $\mathbb R^3$ with  
$$
P(X_{n,1}=\varepsilon_k)=p_{n,k},\quad k=1,2,3,
$$  
and assume  
$$
n p_{n,k} \to \lambda_k,\qquad 0<\lambda_k<\infty,\quad k=1,2,
$$  
(the condition does **not** hold for $k=3$).

(i) Show that $\varphi_{X_n}(t)\to \varphi_{X_\infty}(t)$ for some limiting c.f.

(ii) Identify the distributions of the coordinates of $X_\infty$ and describe the relationship between its first two coordinates.

---

# SURVIVAL GUIDE SOLUTION

---

# (a)(i) Independence implies factorization

### **Claim**  
If $X$ and $Y$ are independent, then  
$$
\varphi_{(X,Y)}(s,t)=\varphi_X(s)\varphi_Y(t).
$$

### **Proof**  
$$
\varphi_{(X,Y)}(s,t)
=E(e^{i(sX+tY)})
=E\big(e^{isX}e^{itY}\big).
$$

By independence of $X$ and $Y$,  
$$
E(e^{isX}e^{itY})=E(e^{isX})\,E(e^{itY})
=\varphi_X(s)\varphi_Y(t).
$$

### **Conclusion**  
C.f. of a joint independent pair factorizes.

### **Key Takeaways**
- Independence of random variables implies independence of measurable transforms such as $e^{isX}$.
- Characteristic functions encode independence via multiplicative factorization.

---

# (a)(ii) Factorization implies independence

### **Claim**  
If  
$$
\varphi_{(X,Y)}(s,t)=\varphi_X(s)\varphi_Y(t)\quad\forall (s,t),
$$  
then $X$ and $Y$ are independent.

### **Proof**  
Construct auxiliary random variables $U,V$ with  
$$
U\stackrel{d}{=}X,\qquad V\stackrel{d}{=}Y,
$$  
and require **$U$ and $V$ to be independent**. Their joint c.f. is  
$$
\varphi_{(U,V)}(s,t)=\varphi_U(s)\varphi_V(t)=\varphi_X(s)\varphi_Y(t).
$$

Given the hypothesis,  
$$
\varphi_{(X,Y)}(s,t)=\varphi_{(U,V)}(s,t).
$$

By uniqueness of characteristic functions,  
$$
(X,Y)\stackrel{d}{=}(U,V).
$$

Since $(U,V)$ is independent, all of its finite-dimensional distributions factorize, hence  
$$
P(X\in A, Y\in B)=P(U\in A,V\in B)=P(U\in A)P(V\in B)=P(X\in A)P(Y\in B).
$$

Thus $X$ and $Y$ are independent.

### **Conclusion**  
Factorization of the joint c.f. is **equivalent** to independence.

### **Key Takeaways**
- Independence can be *transferred* through equality in distribution.  
- Characteristic functions uniquely determine distribution.  
- Proving independence often reduces to comparing c.f.’s.

---

# (b)(i) c.f. of a discrete 3-vector taking only basis vectors

### **Claim**  
For $t=(t_1,t_2,t_3)\in\mathbb R^3$,
$$
\varphi_{X_1}(t)=p_1 e^{it_1}+p_2 e^{it_2}+p_3 e^{it_3}.
$$

### **Proof**  
$$
\varphi_{X_1}(t)
=E(e^{i t\cdot X_1})
=\sum_{k=1}^{3} p_k\, e^{i(t\cdot\varepsilon_k)}.
$$

Dot products:  
$$
t\cdot \varepsilon_1=t_1,\qquad
t\cdot \varepsilon_2=t_2,\qquad
t\cdot \varepsilon_3=t_3.
$$

Thus  
$$
\varphi_{X_1}(t)=p_1e^{it_1}+p_2e^{it_2}+p_3e^{it_3}.
$$

**Distribution of $X_{1,3}$:**  
Since $\varepsilon_1$ and $\varepsilon_2$ have third coordinate $0$,  
$$
X_{1,3}=0 \quad\text{with probability } 1.
$$

### **Conclusion**  
$\varphi_{X_1}(t)$ is a 3-point discrete Fourier transform; third coordinate is degenerate at 0.

### **Key Takeaways**
- For discrete-valued vectors, the c.f. is a weighted exponential sum.  
- Degeneracy of coordinates is immediately visible from support vectors.

---

# (b)(ii) c.f. of the sum of i.i.d. vectors

### **Claim**
$$
\varphi_X(t)=\big(p_1 e^{it_1}+p_2 e^{it_2}+p_3 e^{it_3}\big)^n.
$$

### **Proof**  
Because the $X_m$ are independent and identically distributed:
$$
\varphi_X(t)=\prod_{m=1}^n \varphi_{X_m}(t)
= \big(\varphi_{X_1}(t)\big)^n.
$$

### **Conclusion**
$$
\boxed{\varphi_X(t)=(p_1 e^{it_1}+p_2 e^{it_2}+p_3 e^{it_3})^n.}
$$

### **Key Takeaways**
- c.f. of a sum of independent vectors = product of their c.f.’s.  
- This is a multinomial-type generating function.

---

# (c)(i) Limit of c.f.’s for triangular array with vanishing probabilities

### **Claim**  
$$
\varphi_{X_n}(t)
=\big(p_{n,1} e^{it_1}+p_{n,2} e^{it_2}+p_{n,3} e^{it_3}\big)^n
\;\longrightarrow\;
\exp\!\big(\lambda_1(e^{it_1}-1)+\lambda_2(e^{it_2}-1)\big).
$$

### **Proof**

From (b)(ii),
$$
\varphi_{X_n}(t)
=\left(1 + (e^{it_1}-1)p_{n,1} + (e^{it_2}-1)p_{n,2}\right)^n,
$$
because $e^{it_3}=1$ and $p_{n,3}=1-p_{n,1}-p_{n,2}$.

Use the limit:
$$
\big(1+\tfrac{a_n}{n}\big)^n \to e^{\lim a_n}
$$
provided $n p_{n,k}\to \lambda_k$.

Rewrite:
$$
\varphi_{X_n}(t)
=\left(1 + \frac{n p_{n,1}}{n}(e^{it_1}-1) + \frac{n p_{n,2}}{n}(e^{it_2}-1)\right)^n.
$$

Letting $n\to\infty$,
$$
\varphi_{X_n}(t)
\to \exp\big(\lambda_1(e^{it_1}-1)+\lambda_2(e^{it_2}-1)\big).
$$

This is the c.f. of a **pair of independent Poisson variables** in coordinates 1 and 2.

### **Conclusion**
A limiting random vector $X_\infty$ exists with c.f.
$$
\varphi_{X_\infty}(t)=\exp\big(\lambda_1(e^{it_1}-1)+\lambda_2(e^{it_2}-1)\big).
$$

### **Key Takeaways**
- Vanishing probabilities in triangular arrays produce Poisson limits.  
- Third coordinate disappears because its corresponding probability does not scale as $1/n$.  
- This is a multivariate version of the classical Poisson limit theorem.

---

# (c)(ii) Distribution and relationships of coordinates of $X_\infty$

### **Claim**  
- $X_{\infty,1}\sim\text{Poisson}(\lambda_1)$  
- $X_{\infty,2}\sim\text{Poisson}(\lambda_2)$  
- $X_{\infty,3}=0$ almost surely  
- $X_{\infty,1}$ and $X_{\infty,2}$ are independent

### **Proof**

1. **Independence of first two coordinates.**  
The limiting c.f. factors:
$$
\varphi_{X_\infty}(t)
= \exp(\lambda_1(e^{it_1}-1))\;
  \exp(\lambda_2(e^{it_2}-1)).
$$
By part (a)(ii), factorization of a joint c.f. implies independence.

2. **Marginal distributions.**  
The c.f.  
$$
\exp(\lambda_k(e^{it_k}-1))
$$
is exactly the c.f. of $\text{Poisson}(\lambda_k)$.

3. **Third coordinate.**  
Because every $X_{n,1}$ third coordinate is always $0$, all sums satisfy $X_{n,3}=0$.  
Therefore the limit must satisfy  
$$
X_{\infty,3}=0 \quad\text{a.s.}
$$

### **Conclusion**  
$$
X_\infty = 
(\text{Poisson}(\lambda_1),\text{Poisson}(\lambda_2),0),
$$
with the Poisson components **independent**.

### **Key Takeaways**
- Independence in the limit follows from factorization of the limiting c.f.  
- Poisson limits appear when $np_{n,k}\to\lambda_k$.  
- Degenerate coordinates remain degenerate in the limit.

