---
title: "2024-Q6: Gaussian Random Fields via Hilbert Space Expansions"
permalink: /probability/prelim/2024q6/
tags: [probability-prelim, gaussian-processes, hilbert-spaces]
---

## Short Introduction

This problem is testing the following ideas:

- Understanding how Gaussian random fields can be constructed from simpler Gaussian objects.
- Comfort moving between **integral definitions** and **orthonormal basis expansions** in Hilbert spaces.
- Recognizing when a stochastic process is fully characterized by its mean and covariance.

Main tools likely needed:

- Independence and covariance computations for Brownian motion.
- Orthonormal bases and Parseval’s identity in $L^2$.
- Kolmogorov’s convergence criterion for series of random variables.
- Basic facts about Gaussian random variables and Gaussian processes.

Pattern recognition cues:

- Series of the form $\sum \langle f,\varphi_k\rangle Z_k$ almost always want Kolmogorov’s summable variance criterion.
- Equality in distribution for Gaussian objects usually reduces to matching means and covariances.

---

## Problem Statement

> **Problem 6.** Let $\{B_1(t), B_2(t), t \ge 0\}$ be 2 independent standard Brownian motions (SBM).  
> Let $\Omega = \{(t,u) \in \mathbb{R}^2 : 0 \le t, u \le 1\}$, and let  
> $H = L^2(\Omega,\mathcal{B},\lambda)$, where $\mathcal{B}$ is the Borel $\sigma$-algebra, and $\lambda$ is the Lebesgue measure.  
> Let
> $$
> \langle f,g\rangle = \int_0^1 \int_0^1 f(t,u)\cdot g(t,u)\,dt\,du, \quad f,g \in H.
> $$
> In what follows $(t,u)\in\Omega$.
>
> **(a)** Let $B(t,u)=B_1(t)\cdot B_2(u)$. Calculate:
> 1. $E(B(t,u))$,
> 2. $\operatorname{Cov}[B(t_1,u_1),B(t_2,u_2)]$.
>
> **(b)** Let $\{\varphi_k\}_{k\ge1}\subset H$ be a complete orthonormal basis of $H$, namely  
> $\langle\varphi_k,\varphi_m\rangle = 1$ if $k=m$, and $0$ if $k\neq m$,
> $$
> f=\sum_{k=1}^\infty \langle f,\varphi_k\rangle \varphi_k,
> \quad
> \langle f,g\rangle=\sum_{k=1}^\infty \langle f,\varphi_k\rangle\langle g,\varphi_k\rangle,
> \quad \forall f,g\in H.
> $$
> Let $\{Z_k\}_{k\ge1}$ be independent standard normal random variables.  
> Define
> $$
> T(f)=\sum_{k=1}^\infty \langle f,\varphi_k\rangle Z_k,\quad f\in H.
> $$
> 1. Verify that the series converges almost surely.
> 2. Prove that $\langle f,g\rangle=\operatorname{Cov}[T(f),T(g)]$.
> 3. How is $T(f)$ distributed?
>
> **(c)** For $(t,u)\in\Omega$, let
> $$
> f_{t,u}(x,y)=
> \begin{cases}
> 1, & 0\le x\le t,\ 0\le y\le u,\\
> 0, & \text{otherwise}.
> \end{cases}
> $$
> 1. Prove $E(B(t,u))=E(T(f_{t,u}))$ and
> $$
> \operatorname{Cov}[B(t_1,u_1),B(t_2,u_2)]
> =
> \operatorname{Cov}[T(f_{t_1,u_1}),T(f_{t_2,u_2})].
> $$
> 2. Is $B(t,u)=T(f_{t,u})$ in distribution? Explain.

---

## Solutions by Part

### Part (a)

**Claim.**
$$
E[B(t,u)] = 0,
\qquad
\operatorname{Cov}[B(t_1,u_1),B(t_2,u_2)]
=
(t_1\wedge t_2)(u_1\wedge u_2).
$$

**Proof.**

(i) By definition,
$$
E[B(t,u)] = E[B_1(t)B_2(u)].
$$
Since $B_1$ and $B_2$ are independent,
$$
E[B_1(t)B_2(u)] = E[B_1(t)]E[B_2(u)] = 0,
$$
because each Brownian motion has mean zero.

(ii) By definition of covariance,
$$
\operatorname{Cov}(B(t_1,u_1),B(t_2,u_2))
=
E[B_1(t_1)B_2(u_1)B_1(t_2)B_2(u_2)].
$$
Using independence of $B_1$ and $B_2$,
$$
=E[B_1(t_1)B_1(t_2)]E[B_2(u_1)B_2(u_2)]
=(t_1\wedge t_2)(u_1\wedge u_2).
$$

**Conclusion.**
The process $B(t,u)$ has zero mean and a product-type covariance kernel.

**Key Takeaways.**
- Independence allows factorization of expectations.
- Brownian covariance is $\min(s,t)$.
- Products of independent Gaussian processes remain Gaussian.

---

### Part (b)(i)

**Claim.**
The series $\sum_{k=1}^\infty \langle f,\varphi_k\rangle Z_k$ converges almost surely.

**Proof.**

Let $a_k=\langle f,\varphi_k\rangle$ and $X_k=a_k Z_k$.
Then $E[X_k]=0$ and
$$
\operatorname{Var}(X_k)=a_k^2.
$$
By Parseval,
$$
\sum_{k=1}^\infty a_k^2 = \|f\|_H^2 < \infty.
$$
Thus $\sum_k \operatorname{Var}(X_k)<\infty$.
By Kolmogorov’s summable variance (two-series) criterion,
$\sum_k X_k$ converges almost surely.

**Conclusion.**
$T(f)$ is well defined for all $f\in H$.

**Key Takeaways.**
- Square-summable coefficients are the key condition.
- Mean-zero + summable variances ⇒ a.s. convergence.

---

### Part (b)(ii)

**Claim.**
$$
\operatorname{Cov}[T(f),T(g)] = \langle f,g\rangle.
$$

**Proof.**

Since $E[Z_k]=0$,
$$
E[T(f)]=0.
$$
Then
$$
\operatorname{Cov}(T(f),T(g))=E[T(f)T(g)].
$$
Expanding,
$$
E\left[\sum_{i}\sum_{j}\langle f,\varphi_i\rangle
\langle g,\varphi_j\rangle Z_i Z_j\right].
$$
Independence gives $E[Z_iZ_j]=\delta_{ij}$, so only diagonal terms remain:
$$
=\sum_{k=1}^\infty \langle f,\varphi_k\rangle\langle g,\varphi_k\rangle.
$$
By Parseval, this equals $\langle f,g\rangle$.

**Conclusion.**
The covariance structure of $T$ exactly matches the Hilbert inner product.

**Key Takeaways.**
- Independence kills off cross terms.
- Parseval is the bridge between geometry and probability.

---

### Part (b)(iii)

**Claim.**
$$
T(f)\sim \mathcal{N}(0,\|f\|_H^2).
$$

**Proof.**

Finite partial sums
$$
\sum_{k=1}^N \langle f,\varphi_k\rangle Z_k
\sim \mathcal{N}\left(0,\sum_{k=1}^N \langle f,\varphi_k\rangle^2\right).
$$
As $N\to\infty$, the variance converges to $\|f\|_H^2$.
Almost sure convergence implies convergence in distribution.

**Conclusion.**
$T(f)$ is centered Gaussian with variance $\|f\|_H^2$.

**Key Takeaways.**
- Gaussianity is preserved under summation.
- Variance comes from Parseval.

---

### Part (c)(i)

**Claim.**
Means and covariances of $B(t,u)$ and $T(f_{t,u})$ agree.

**Proof.**

From part (a), $E[B(t,u)]=0$.
From part (b)(ii), $E[T(f_{t,u})]=0$.

For covariance,
$$
\operatorname{Cov}(T(f_{t_1,u_1}),T(f_{t_2,u_2}))
=\langle f_{t_1,u_1},f_{t_2,u_2}\rangle.
$$
This inner product equals
$$
\int_0^{t_1\wedge t_2}\int_0^{u_1\wedge u_2}1\,dx\,dy
=(t_1\wedge t_2)(u_1\wedge u_2),
$$
matching part (a).

**Conclusion.**
The two processes have identical first and second moments.

**Key Takeaways.**
- Indicator functions encode covariance structure.
- Inner products compute overlap volumes.

---

### Part (c)(ii)

**Claim.**
$B(t,u)$ and $T(f_{t,u})$ are equal in distribution.

**Proof.**

Both collections are Gaussian processes.
Gaussian laws are completely determined by mean and covariance.
Since both match, the processes coincide in distribution.

**Conclusion.**
Yes, $B(t,u)\overset{d}{=}T(f_{t,u})$.

**Key Takeaways.**
- For Gaussians: mean + covariance = full law.
- This is a canonical construction of a Gaussian random field.

---

## Master Key Takeaways

- Hilbert space expansions give canonical Gaussian processes.
- Parseval identity is the engine behind covariance matching.
- Kolmogorov’s criterion is the correct tool for random series.

## Cheat Sheet Entries to Extract

- **Summable Variance Criterion:** If $\sum \operatorname{Var}(X_k)<\infty$ and $E[X_k]=0$, then $\sum X_k$ converges a.s.
- **Gaussian Process Equality:** Two Gaussian processes are equal in law iff their means and covariances match.
- **Parseval:** $\|f\|^2=\sum \|\langle f,\varphi_k\rangle\|^2$.

## Notes on My Original Work

- Strong: Correct identification of Kolmogorov’s criterion and Parseval.
- Needed patching: Explicit justification that off-diagonal covariance terms vanish.
- Overall approach was correct and exam-appropriate.
