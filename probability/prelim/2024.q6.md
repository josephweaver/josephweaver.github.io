# 2024.Q6 – Brownian Motion, Wiener Integrals, and Gaussian Series

[2024 Probability Prelim Exam (PDF)](https://stt.natsci.msu.edu/_assets/files/graduate-programs/Prelim%20Prob%20Exam%202024.pdf)

## **Problem Statement (reconstructed from the exam + your work)**

Let $\{B_t^1\}_{t\ge0}$ and $\{B_t^2\}_{t\ge0}$ be two independent standard Brownian motions on a probability space $(\Omega,\mathcal F,\mathbf P)$.

Let
$$
H = L^2\big([0,1],\mathcal B([0,1]),\lambda\big)
$$
be the Hilbert space of (equivalence classes of) square–integrable functions on $[0,1]$, with inner product
$$
\langle f,g\rangle = \int_0^1 f(t)g(t)\,dt.
$$

---

### **(a)**  

Let
$$
X := \int_0^1 B_t\,dt
$$
for a standard Brownian motion $B$. For fixed $s\in[0,1]$, compute

(i) $\displaystyle E[B_s X]$.

(ii) $\displaystyle E[X^2]$.

---

### **(b)**  

Let $\{e_k\}_{k\ge1}$ be a complete orthonormal basis of $H$, and let $\{\xi_k\}_{k\ge1}$ be i.i.d. standard normal random variables, independent of everything else. For $f\in H$ define the random series
$$
T(f) := \sum_{k=1}^\infty \xi_k\,\langle f,e_k\rangle.
$$

(i) Show that for every fixed $f\in H$, the series for $T(f)$ converges almost surely.  

(ii) Show that for all $f,g\in H$,
$$
E[T(f)] = 0, 
\qquad
E[T(f)T(g)] = \langle f,g\rangle.
$$

(iii) Identify the distribution of $T(f)$ (for fixed $f$).

---

### **(c)**  

For $f\in H$, define the **Wiener integral**
$$
I(f) := \int_0^1 f(t)\,dB_t,
$$
where $B$ is a standard Brownian motion.

(i) Show that for all $f,g\in H$,
$$
E[I(f)] = 0,
\qquad
\mathrm{Cov}(I(f),I(g)) = E[I(f)I(g)] = \langle f,g\rangle.
$$

(ii) Compare the families $\{I(f)\}_{f\in H}$ and $\{T(f)\}_{f\in H}$.  
Are they equal in distribution as Gaussian families indexed by $H$? Explain.

---

# **Part (a)**

## **(a)(i) Compute $E[B_s X]$**

### **Claim.**
For $s\in[0,1]$,
$$
E[B_s X] 
= E\!\left[B_s\int_0^1 B_t\,dt\right]
= s - \frac{s^2}{2}.
$$

### **Proof.**

By Fubini/Tonelli (everything is square–integrable),
$$
E[B_s X]
= E\!\left[B_s\int_0^1 B_t\,dt\right]
= \int_0^1 E[B_s B_t]\,dt.
$$

For standard Brownian motion,
$$
E[B_s B_t] = \min(s,t).
$$

Thus
$$
\int_0^1 E[B_s B_t]\,dt
= \int_0^1 \min(s,t)\,dt
= \int_0^s t\,dt + \int_s^1 s\,dt
= \frac{s^2}{2} + s(1-s)
= s - \frac{s^2}{2}.
$$

### **Conclusion.**
$$
E[B_s X] = s - \frac{s^2}{2}.
$$

### **Key Takeaways**
- Use **Fubini** to move expectation inside integrals.
- Brownian covariance identity: $E[B_sB_t]=s\wedge t$.
- Break integrals at the point where the “min” switches: $t=s$.

---

## **(a)(ii) Compute $E[X^2]$**

### **Claim.**
$$
E[X^2]
= E\!\left[\left(\int_0^1 B_t\,dt\right)^2\right]
= \frac{1}{3}.
$$

### **Proof.**

Again by Fubini/Tonelli,
$$
E[X^2]
= E\left[\int_0^1 B_s\,ds \int_0^1 B_t\,dt\right]
= \int_0^1\int_0^1 E[B_sB_t]\,dt\,ds
= \int_0^1\int_0^1 (s\wedge t)\,dt\,ds.
$$

Compute:
$$
\int_0^1\int_0^1 (s\wedge t)\,dt\,ds
= 2\int_0^1\int_0^t s\,ds\,dt
\quad\text{(by symmetry in $s,t$)}.
$$

Now
$$
\int_0^t s\,ds = \frac{t^2}{2},
$$
so
$$
2\int_0^1\frac{t^2}{2}\,dt = \int_0^1 t^2\,dt = \frac{1}{3}.
$$

### **Conclusion.**
$$
E[X^2] = \frac{1}{3}.
$$

### **Key Takeaways**
- For double integrals with a symmetric kernel $s\wedge t$, it is often easier to integrate over the triangle $s<t$ and multiply by 2.
- This kind of calculation is a **prototype** for computing variances of Brownian integrals.

---

# **Part (b)**

We now work purely in the Hilbert space $H$ with an abstract Gaussian series.

---

## **(b)(i) Almost sure convergence of $T(f)$**

### **Claim.**
For each fixed $f\in H$,
$$
T(f) = \sum_{k=1}^\infty \xi_k\,\langle f,e_k\rangle
$$
converges almost surely.

### **Proof.**

Let
$$
a_k := \langle f,e_k\rangle, \quad Y_k := \xi_k a_k.
$$
Then $\{Y_k\}$ are independent, mean–zero random variables with
$$
E[Y_k^2] = a_k^2 E[\xi_k^2] = a_k^2.
$$

Because $\{e_k\}$ is an orthonormal basis of $H$,
$$
\sum_{k=1}^\infty a_k^2 = \sum_{k=1}^\infty |\langle f,e_k\rangle|^2 = \|f\|_H^2 < \infty.
$$

We apply **Kolmogorov’s two–series theorem** to $\{Y_k\}$. It suffices to check:

1. $\displaystyle \sum_k E[Y_k^2] < \infty$ (already done).  
2. $\displaystyle \sum_k P(|Y_k| > 1) < \infty.$

For (2), use Chebyshev:
$$
P(|Y_k|>1)
\le E[Y_k^2]
= a_k^2.
$$
Thus
$$
\sum_{k=1}^\infty P(|Y_k|>1)
\le \sum_{k=1}^\infty a_k^2
= \|f\|_H^2 < \infty.
$$

Kolmogorov’s two–series theorem now gives that
$$
\sum_{k=1}^\infty Y_k = \sum_{k=1}^\infty \xi_k\langle f,e_k\rangle
$$
converges almost surely.

### **Conclusion.**
For each $f\in H$, $T(f)$ is a well–defined (a.s. finite) random variable.

### **Key Takeaways**
- In an orthonormal basis, $\sum |\langle f,e_k\rangle|^2 = \|f\|^2$ is your **variance budget**.
- Kolmogorov’s **two–series theorem** is tailor–made for convergence of random series of independent terms.
- Chebyshev’s inequality is a quick way to control tail probabilities $\sum P(|Y_k|>1)$.

---

## **(b)(ii) Mean and covariance of $T(f)$**

### **Claim.**
For all $f,g\in H$,
$$
E[T(f)] = 0,
\qquad
E[T(f)T(g)] = \langle f,g\rangle.
$$

### **Proof.**

Write
$$
T(f) = \sum_{k=1}^\infty \xi_k\,\langle f,e_k\rangle,
\qquad
T(g) = \sum_{j=1}^\infty \xi_j\,\langle g,e_j\rangle.
$$

Since each $\xi_k$ is centered and the series converges in $L^2$,
$$
E[T(f)] = \sum_{k=1}^\infty E[\xi_k]\langle f,e_k\rangle = 0.
$$

For the covariance,
$$
E[T(f)T(g)]
= E\left[\sum_{k,j} \xi_k\xi_j \langle f,e_k\rangle \langle g,e_j\rangle\right].
$$

Using independence and $E[\xi_k\xi_j] = \delta_{kj}$ (Kronecker delta),
$$
E[\xi_k\xi_j] = 
\begin{cases}
1,& k=j,\\
0,& k\ne j,
\end{cases}
$$
we get
$$
E[T(f)T(g)]
= \sum_{k,j} \langle f,e_k\rangle \langle g,e_j\rangle E[\xi_k\xi_j]
= \sum_{k=1}^\infty \langle f,e_k\rangle \langle g,e_k\rangle.
$$

Finally, because $\{e_k\}$ is ONB,
$$
\sum_{k=1}^\infty \langle f,e_k\rangle \langle g,e_k\rangle
= \langle f,g\rangle.
$$

### **Conclusion.**
$T$ is a **centered Gaussian linear functional** on $H$ with covariance
$$
\mathrm{Cov}(T(f),T(g)) = \langle f,g\rangle.
$$

### **Key Takeaways**
- Gaussian series with independent standard normals + ONB give a **canonical isonormal Gaussian process** on $H$.
- The covariance structure is exactly the inner product structure of the underlying Hilbert space.

---

## **(b)(iii) Distribution of $T(f)$**

### **Claim.**
For fixed $f\in H$,
$$
T(f) \sim N\big(0,\|f\|_H^2\big).
$$

### **Proof.**

We already have:

- $E[T(f)] = 0$,
- $\mathrm{Var}(T(f)) = E[T(f)^2] = \langle f,f\rangle = \|f\|_H^2.$

Because $T(f)$ is a limit of **finite linear combinations** of independent Gaussian random variables,
$$
T(f) = \lim_{N\to\infty}\sum_{k=1}^N \xi_k\langle f,e_k\rangle
$$
and every finite sum on the right is Gaussian, the limit (in $L^2$ and a.s.) is also Gaussian.

Thus $T(f)$ is a centered Gaussian with variance $\|f\|^2$.

### **Conclusion.**
$$
T(f) \sim \mathcal N(0,\|f\|^2).
$$

### **Key Takeaways**
- Any (possibly infinite) linear combination of independent Gaussians, when convergent in $L^2$, is again Gaussian.
- For Gaussian linear functionals, **mean and variance** completely identify the one-dimensional distributions.

---

# **Part (c)**

Now we relate the abstract Gaussian functional $T$ to actual Brownian integrals.

---

## **(c)(i) Mean and covariance of the Wiener integral**

### **Claim.**
For all $f,g\in H$,
$$
E[I(f)] = 0,
\qquad
E[I(f)I(g)] = \langle f,g\rangle.
$$

### **Proof.**

First recall the definition of the Wiener integral $I(f)$ for simple functions, then extend by density.

1. **Simple functions.**  
   Suppose
   $$
   f(t) = \sum_{i=1}^m a_i \mathbf{1}_{(t_{i-1},t_i]}(t),
   \quad
   0 = t_0 < t_1 < \cdots < t_m \le 1.
   $$
   Then
   $$
   I(f) := \int_0^1 f(t)\,dB_t
   = \sum_{i=1}^m a_i (B_{t_i}-B_{t_{i-1}}).
   $$
   Similarly for $g(t) = \sum_j b_j \mathbf{1}_{(s_{j-1},s_j]}$.

   - Mean:
     $$
     E[I(f)] = \sum_i a_i E[B_{t_i}-B_{t_{i-1}}] = 0.
     $$

   - Covariance:
     $$
     E[I(f)I(g)]
     = \sum_{i,j} a_i b_j E\!\big[(B_{t_i}-B_{t_{i-1}})(B_{s_j}-B_{s_{j-1}})\big].
     $$

     But for Brownian motion,
     $$
     E[(B_{t_i}-B_{t_{i-1}})(B_{s_j}-B_{s_{j-1}})]
     = \lambda\big((t_{i-1},t_i]\cap(s_{j-1},s_j]\big),
     $$
     i.e. the length of the overlap of the two intervals. Hence
     $$
     E[I(f)I(g)]
     = \sum_{i,j} a_i b_j \lambda\big((t_{i-1},t_i]\cap(s_{j-1},s_j]\big)
     = \int_0^1 f(t)g(t)\,dt
     = \langle f,g\rangle.
     $$

2. **General $f,g\in H$.**  
   Approximate $f,g$ in $L^2$ by simple functions $f_n,g_n$.  
   The mapping $f\mapsto I(f)$ is an isometry in $L^2(\Omega)$, so
   $$
   E[(I(f_n)-I(f))^2] = \|f_n-f\|_H^2 \to 0,
   $$
   and similarly for $g$. Then by continuity of covariance in $L^2$ we can pass to the limit:
   $$
   E[I(f)I(g)] = \langle f,g\rangle,
   \quad
   E[I(f)] = 0.
   $$

### **Conclusion.**
The Wiener integral $I: H\to L^2(\Omega)$ is a centered Gaussian linear functional with
$$
\mathrm{Cov}(I(f),I(g)) = \langle f,g\rangle.
$$

### **Key Takeaways**
- For Brownian integrals of simple functions, covariance reduces to computing **overlaps of time intervals**:
  $\lambda((a,b]\cap(c,d])$.
- Extending from simple functions to $L^2$ uses **density and isometry**.
- The covariance structure of Wiener integrals matches the inner product of $H$.

---

## **(c)(ii) Comparing $\{I(f)\}$ and $\{T(f)\}$**

### **Claim.**
The families $\{I(f)\}_{f\in H}$ and $\{T(f)\}_{f\in H}$ have the **same distribution** as centered Gaussian families on $H$. In particular, for any finite collection $f_1,\dots,f_n\in H$,
$$
(I(f_1),\dots,I(f_n)) \stackrel{d}{=} (T(f_1),\dots,T(f_n)).
$$

### **Proof.**

Both $\{I(f)\}$ and $\{T(f)\}$ are **centered Gaussian families** indexed by $H$:

- Linearity in $f$: both $I$ and $T$ are linear maps $H\to L^2(\Omega)$.
- For every finite choice $f_1,\dots,f_n$, each vector
  $$
  (I(f_1),\dots,I(f_n)),
  \quad
  (T(f_1),\dots,T(f_n))
  $$
  is multivariate Gaussian, as linear combinations of underlying normal variables.

From (b)(ii) and (c)(i), for all $f,g\in H$,
$$
E[T(f)] = E[I(f)] = 0,
$$
and
$$
E[T(f)T(g)] = \langle f,g\rangle = E[I(f)I(g)].
$$

For Gaussian vectors, **mean vector and covariance matrix completely determine the law**. Hence, for every finite family $\{f_1,\dots,f_n\}$,
$$
(I(f_1),\dots,I(f_n)) \text{ and } (T(f_1),\dots,T(f_n))
$$
have the same joint distribution.

Therefore the two index families $\{I(f)\}$ and $\{T(f)\}$ are equal in distribution as Gaussian families indexed by $H$.

### **Conclusion.**
The Wiener integral $I$ and the abstract Gaussian series construction $T$ are *two realizations of the same isonormal Gaussian process* over $H$. They have identical finite–dimensional distributions, hence the same law.

### **Key Takeaways**
- For **Gaussian** processes, *matching all covariances and means* is enough to guarantee equality in distribution.
- The “orthonormal basis + standard normals” construction is the abstract version of the Wiener integral with respect to Brownian motion.
- A common confusion (and what you were wrestling with):  
  For *non-Gaussian* families, matching first two moments is **not** enough.  
  For *Gaussian* families, it **is** enough.

---

# **Big Picture for Question 6**

- You built two versions of an **isonormal Gaussian process** on the Hilbert space $H=L^2[0,1]$:
  - One via Brownian motion (Wiener integrals $I(f)$),
  - One via ONB and i.i.d. normals ($T(f)$).
- You showed:
  - Both are centered,
  - Both have covariance $\langle f,g\rangle$,
  - Both are Gaussian and linear in $f$.
- Therefore they are **the same process in law**.

This question is a template for recognizing when two different–looking constructions are secretly the same Gaussian object.
