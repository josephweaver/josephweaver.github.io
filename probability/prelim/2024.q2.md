# 2024.Q2 – Heavy-Tailed Sums, Truncation, and Almost Sure Convergence
---

## **Problem Statement (verbatim)** :contentReference[oaicite:0]{index=0}  

Let $X, X_1, X_2, \ldots$ be i.i.d. sequence of positive random variables. Let $0 < \beta < 1$. Assume  
$$
P(X > x) \le x^{-\beta}, \quad x > 1.
$$

Let $\{a_n\}_{n=1,2,\dots}$ be a sequence of positive real numbers that satisfies  
$$
\sum_{n=1}^\infty a_n^\beta < \infty.
$$  
Prove the following:

**(a)** $\displaystyle \sum_{n=1}^\infty P(a_n X > 1) < \infty.$

**(b)(i)** $\displaystyle \sum_{n=1}^\infty E\big(a_n X \cdot \mathbf{1}_{\{a_n X < 1\}}\big) < \infty.$

**(b)(ii)** $\displaystyle \sum_{n=1}^\infty E\big(a_n^2 X^2 \cdot \mathbf{1}_{\{a_n X < 1\}}\big) < \infty.$

**(c)**  
(i) $\displaystyle \sum_{n=1}^\infty a_n X_n < \infty,$ a.s.  

(ii) Assume also that $\{a_n\}_{n=1,2,\dots}$ is non-decreasing. Prove:  
$$
a_n \cdot \sum_{k=1}^n X_k \longrightarrow 0, \quad \text{a.s.}
$$

---

# **Part (a)**

### **Claim.**  
$$
\sum_{n=1}^\infty P(a_n X > 1) < \infty.
$$

### **Proof.**

For each $n$,
$$
P(a_n X > 1) = P\!\left(X > \frac{1}{a_n}\right).
$$

We consider two cases.

- **Case 1: $a_n < 1$.** Then $\frac{1}{a_n} > 1$, so we can use the tail bound:
  $$
  P\!\left(X > \frac{1}{a_n}\right)
  \le \left(\frac{1}{a_n}\right)^{-\beta}
  = a_n^\beta.
  $$

- **Case 2: $a_n \ge 1$.** Then $\frac{1}{a_n} \le 1$, so we just use the trivial bound
  $$
  P\!\left(X > \frac{1}{a_n}\right) \le 1 \le a_n^\beta
  $$
  because $a_n^\beta \ge 1$ when $a_n \ge 1$.

In both cases,
$$
P(a_n X > 1) \le a_n^\beta.
$$
Therefore
$$
\sum_{n=1}^\infty P(a_n X > 1) \le \sum_{n=1}^\infty a_n^\beta < \infty.
$$

### **Conclusion.**  
The series $\sum_n P(a_n X > 1)$ converges, controlled by the given summability of $\sum a_n^\beta$.

### **Key Takeaways**
- It is often useful to **manipulate inside the probability** first, and then apply given tail bounds.
- Splitting into cases $a_n < 1$ and $a_n \ge 1$ lets you combine the **nontrivial tail bound** with the **trivial inequality** $P(\cdot)\le 1$.
- The condition $\sum a_n^\beta < \infty$ not only gives summability, it also implies $a_n \to 0$.

---

# **Part (b)(i)**

### **Claim.**  
$$
\sum_{n=1}^\infty E\big[a_n X \,\mathbf{1}_{\{a_n X < 1\}}\big] < \infty.
$$

### **Proof.**

Let $Y_n = a_n X$. Then
$$
E\big[Y_n \mathbf{1}_{\{Y_n<1\}}\big]
= \int_0^1 P(Y_n > t)\,dt
= \int_0^1 P\!\left(X > \frac{t}{a_n}\right) dt.
$$

We split the integral at $t = a_n$:

1. **Region $0 \le t \le a_n$:**  
   Here $\frac{t}{a_n} \le 1$, so we only know
   $$
   P\!\left(X > \frac{t}{a_n}\right) \le 1.
   $$
   Hence
   $$
   \int_0^{a_n} P\!\left(X > \frac{t}{a_n}\right) dt \le \int_0^{a_n} 1\,dt = a_n.
   $$

2. **Region $a_n \le t \le 1$:**  
   Here $\frac{t}{a_n} \ge 1$, so we can use the tail bound:
   $$
   P\!\left(X > \frac{t}{a_n}\right)
   \le \left(\frac{t}{a_n}\right)^{-\beta}
   = a_n^\beta t^{-\beta}.
   $$
   Thus
   $$
   \int_{a_n}^1 P\!\left(X > \frac{t}{a_n}\right) dt
   \le a_n^\beta \int_{a_n}^1 t^{-\beta} dt.
   $$
   For $0<\beta<1$,
   $$
   \int_{a_n}^1 t^{-\beta} dt
   = \frac{1 - a_n^{1-\beta}}{1-\beta}
   \le \frac{1}{1-\beta}.
   $$
   Hence
   $$
   \int_{a_n}^1 P\!\left(X > \frac{t}{a_n}\right) dt
   \le \frac{a_n^\beta}{1-\beta}.
   $$

Combining the two regions,
$$
E[a_n X \mathbf{1}_{\{a_nX<1\}}]
\le a_n + \frac{a_n^\beta}{1-\beta}.
$$

Since $\sum a_n^\beta < \infty$, we get $a_n \to 0$. So for large $n$, $a_n \le a_n^\beta$, which implies
$$
\sum_{n=1}^\infty a_n < \infty
\quad\text{and}\quad
\sum_{n=1}^\infty a_n^\beta < \infty.
$$
Therefore
$$
\sum_{n=1}^\infty E[a_n X \mathbf{1}_{\{a_nX<1\}}]
\le \sum_{n=1}^\infty a_n
+ \frac{1}{1-\beta} \sum_{n=1}^\infty a_n^\beta
< \infty.
$$

### **Conclusion.**  
The series of truncated expectations $\sum_n E[a_nX\,1_{\{a_nX<1\}}]$ converges.

### **Key Takeaways**
- Use the **tail-integral formula**  
  $$
  E[Y1_{\{Y<1\}}] = \int_0^1 P(Y>t)\,dt
  $$
  to turn expectations into integrals of tail probabilities.
- Split the integral at the point where the **given tail bound becomes valid**.
- The given summability $\sum a_n^\beta$ can be used twice: once directly, and once to deduce $a_n \to 0$, which helps to control $\sum a_n$.

---

# **Part (b)(ii)**

### **Claim.**  
$$
\sum_{n=1}^\infty E\big[a_n^2 X^2 \,\mathbf{1}_{\{a_n X < 1\}}\big] < \infty.
$$

### **Proof.**

Let $Z_n = a_n^2 X^2$. By the same tail-integral idea,
$$
E[Z_n \mathbf{1}_{\{Z_n<1\}}]
= \int_0^1 P(Z_n > t)\,dt
= \int_0^1 P\!\left(X > \frac{\sqrt{t}}{a_n}\right) dt.
$$

Again, split at $t=a_n^2$:

1. **Region $0 \le t \le a_n^2$:**  
   Here $\frac{\sqrt{t}}{a_n} \le 1$, so
   $$
   P\!\left(X > \frac{\sqrt{t}}{a_n}\right) \le 1
   \quad\Rightarrow\quad
   \int_0^{a_n^2} P\!\left(X > \frac{\sqrt{t}}{a_n}\right) dt
   \le a_n^2.
   $$

2. **Region $a_n^2 \le t \le 1$:**  
   Here $\frac{\sqrt{t}}{a_n} \ge 1$, so
   $$
   P\!\left(X > \frac{\sqrt{t}}{a_n}\right) 
   \le \left(\frac{\sqrt{t}}{a_n}\right)^{-\beta}
   = a_n^\beta t^{-\beta/2}.
   $$
   Thus
   $$
   \int_{a_n^2}^1 P\!\left(X > \frac{\sqrt{t}}{a_n}\right) dt
   \le a_n^\beta \int_{a_n^2}^1 t^{-\beta/2} dt.
   $$
   Since $0<\beta<2$, the integral is finite and bounded uniformly in $n$:
   $$
   \int_{a_n^2}^1 t^{-\beta/2} dt
   = \frac{1 - (a_n^2)^{1-\beta/2}}{1-\beta/2}
   \le \frac{1}{1-\beta/2}.
   $$

Therefore,
$$
E[a_n^2 X^2 \mathbf{1}_{\{a_nX<1\}}]
\le a_n^2 + \frac{a_n^\beta}{1-\beta/2}.
$$

As before, from $\sum a_n^\beta<\infty$ we get $a_n\to 0$, hence eventually $a_n^2 \le a_n^\beta$. So both $\sum a_n^2$ and $\sum a_n^\beta$ converge, and
$$
\sum_{n=1}^\infty E[a_n^2 X^2 \mathbf{1}_{\{a_nX<1\}}] < \infty.
$$

### **Conclusion.**  
The series of truncated second moments $\sum_n E[a_n^2 X^2 1_{\{a_nX<1\}}]$ is finite.

### **Key Takeaways**
- The same **integral-splitting trick** works for higher powers.
- The exponent $\beta < 2$ guarantees that $\int t^{-\beta/2}\,dt$ is finite near $t=0$.
- Heavy-tail bounds combined with shrinking weights can make even second moments summable.

---

# **Part (c)(i)**

### **Claim.**  
$$
\sum_{n=1}^\infty a_n X_n < \infty \quad \text{a.s.}
$$

### **Proof.**

We want to apply **Kolmogorov’s Three-Series Theorem** to the independent sequence $\{a_n X_n\}$. Define
$$
Y_n = a_n X_n.
$$

The theorem says $\sum Y_n$ converges a.s. if and only if the following three series are all finite:

1. $\displaystyle \sum_{n=1}^\infty P(|Y_n| > 1) < \infty.$
2. $\displaystyle \sum_{n=1}^\infty E\big(Y_n \mathbf{1}_{\{|Y_n|\le 1\}}\big) < \infty.$
3. $\displaystyle \sum_{n=1}^\infty E\big(Y_n^2 \mathbf{1}_{\{|Y_n|\le 1\}}\big) < \infty.$

Here $Y_n \ge 0$ (since $X_n\ge 0$), so $|Y_n| = Y_n$, and $|Y_n|\le1$ is the same as $Y_n<1$.

- Condition (1) is precisely $\sum P(a_n X_n>1) < \infty$, which holds by part (a).
- Condition (2) is $\sum E[a_n X_n \mathbf{1}_{\{a_nX_n\le 1\}}] < \infty$, which we proved in part (b)(i).
- Condition (3) is $\sum E[a_n^2 X_n^2 \mathbf{1}_{\{a_nX_n\le 1\}}] < \infty$, which we proved in part (b)(ii).

Thus all three conditions hold. By Kolmogorov’s Three-Series Theorem,
$$
\sum_{n=1}^\infty a_n X_n \quad\text{converges almost surely.}
$$

### **Conclusion.**  
The weighted series $\sum a_n X_n$ converges a.s. by verifying the three-series conditions using parts (a) and (b).

### **Key Takeaways**
- Kolmogorov’s Three-Series Theorem is a standard tool for **a.s. convergence of sums of independent (possibly heavy-tailed) random variables**.
- Parts (a), (b)(i), and (b)(ii) were all aimed at setting up its three conditions.
- Truncation at level 1 is conventional in the theorem, but any fixed threshold would work up to constants.

---

# **Part (c)(ii)**

> Problem statement: “Assume also that $\{a_n\}_{n=1,2,\dots}$ is non-decreasing. Prove:  
> $$
> a_n \sum_{k=1}^n X_k \to 0 \quad \text{a.s.}
> $$”
>
> In the standard form of the lemma we will use, one actually assumes the weights are **non-increasing** and tend to $0$. This is how the official solution proceeds. So we implicitly use that condition (and you correctly noticed this issue in your reflection). :contentReference[oaicite:1]{index=1}  

### **Claim.**  
Suppose $\{a_n\}$ is **non-increasing** with $a_n \to 0$, and $\sum_{n=1}^\infty a_n X_n$ converges a.s. Then
$$
a_n \sum_{k=1}^n X_k \longrightarrow 0 \quad \text{a.s.}
$$

### **Proof. (Kronecker’s lemma)**

Let $b_n = a_n$, and define
$$
S_n = \sum_{k=1}^n X_k.
$$

We know from part (c)(i) that
$$
\sum_{n=1}^\infty b_n X_n
= \sum_{n=1}^\infty a_n X_n
$$
converges a.s.

A standard version of **Kronecker’s Lemma** states:

> If $\{b_n\}$ is a non-increasing sequence of positive numbers with $b_n \to 0$, and $\sum b_n x_n$ converges, then
> $$
> b_n \sum_{k=1}^n x_k \to 0.
> $$

Apply this lemma with $x_k = X_k$ and $b_n = a_n$. Since the weighted sum converges a.s. and $a_n \downarrow 0$, we obtain
$$
a_n S_n \to 0 \quad \text{a.s.}
$$

### **Conclusion.**  
Under the usual monotone-decreasing assumption on the weights, convergence of $\sum a_n X_n$ implies that the *scaled partial sums* $a_n \sum_{k=1}^n X_k$ vanish a.s.

### **Key Takeaways**
- Kronecker’s Lemma allows you to go from
  > “$\sum a_n X_n$ converges”
  to  
  > “$a_n \sum_{k=1}^n X_k \to 0$”  
  when $\{a_n\}$ is **decreasing** and $a_n \to 0$.
- In problems like this, if you see both:
  - a convergent **weighted sum** $\sum a_n X_n$, and  
  - a question about **scaled partial sums** $a_n S_n$,  
  you should immediately think of **Kronecker’s Lemma**.
- It is important to check the **monotonicity direction**: Kronecker’s Lemma is usually stated for non-increasing weights that tend to zero.

---

# **Global Key Takeaways for Question 2**

- Heavy-tail bounds like $P(X>x)\le x^{-\beta}$ plus a summability condition on weights $\{a_n\}$ are a classic setup for:
  - controlling tail probabilities $\sum P(a_nX>1)$,
  - controlling truncated expectations via tail integrals,
  - applying Kolmogorov’s Three-Series Theorem.
- **Integral representation of expectations** (tail integrals) is extremely powerful for bounding $E[g(X)]$ when we are given information about tail probabilities $P(X>x)$.
- Once you show that the weighted sum $\sum a_n X_n$ converges a.s., **Kronecker’s Lemma** is the right tool to deduce that the scaled partial sums vanish.
- This problem is an archetype of how to handle:
  - heavy-tailed variables,
  - truncation + summability,
  - three-series theorem,
  - and Kronecker’s lemma in one coherent package.
