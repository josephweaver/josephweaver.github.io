---
title: "Prob_F22 – Question 5 (Martingale Normalization via Quadratic Variation)"
exam: "MSU Probability Prelim – August 2022"
tags: [martingale, quadratic variation, predictable process, Kronecker]
---

[2022 Probability Prelim Exam (PDF)](https://stt.natsci.msu.edu/_assets/files/graduate-programs/Prelim%20Prob%20Exam%202022.pdf)

## Problem 5 (verbatim)

Let $\{X_k,\mathcal F_k\}$ be a martingale sequence with $\mathbb E(X_k^2)<\infty$, $k=0,1,\dots$.
Let
$$
A_n^X=\sum_{k=1}^n \mathbb E_{\mathcal F_{k-1}}\bigl(X_k-X_{k-1}\bigr)^2,
\qquad n=1,2,\dots
$$
(The sequence $\{A_n^X\}$ is known as the *predictable and increasing process* associated with
$\{X_k,\mathcal F_k\}$.)  
Assume $A_1^X=C>0$.

### a.
Let
$$
Y_n=\sum_{k=1}^n \frac{X_k-X_{k-1}}{A_k^X},\qquad n=1,2,\dots
$$

(i) Prove that
$$
\mathbb E_{\mathcal F_{k-1}}(Y_k-Y_{k-1})^2
=\frac{A_k^X-A_{k-1}^X}{(A_k^X)^2}
\le \frac{1}{C}
\quad\text{a.s.}
$$

(ii) Prove that $\{Y_k,\mathcal F_k\}$ is a martingale and that $\mathbb E(Y_k^2)<\infty$.

For the rest of the problem define
$$
A_n^Y=\sum_{k=1}^n \mathbb E_{\mathcal F_{k-1}}(Y_k-Y_{k-1})^2.
$$

### b.
(i) Prove that
$$
A_n^Y \le \int_C^\infty x^{-2}\,dx = C^{-1},
\quad \text{a.s.}
$$

(ii) Prove that
$$
\sup_{n\ge1}\mathbb E(Y_n^2)<\infty
\quad\text{and}\quad
\lim_{n\to\infty}Y_n \text{ exists and is finite a.s.}
$$

### c.
Show how to use **Kronecker’s Lemma** to prove that if $A_n^X\to\infty$ a.s., then
$$
\frac{X_n}{A_n^X}\to 0 \quad \text{a.s.}
$$

---

## Solution

### a(i)

#### Claim
$$
\mathbb E_{\mathcal F_{k-1}}(Y_k-Y_{k-1})^2
=\frac{A_k^X-A_{k-1}^X}{(A_k^X)^2}
\le \frac{1}{C}
\quad\text{a.s.}
$$

#### Proof

Start by explicitly computing the increment:
$$
Y_k-Y_{k-1}
=\sum_{m=1}^k \frac{X_m-X_{m-1}}{A_m^X}
-\sum_{m=1}^{k-1} \frac{X_m-X_{m-1}}{A_m^X}
=\frac{X_k-X_{k-1}}{A_k^X}.
$$

Now substitute into the conditional second moment:
$$
\mathbb E_{\mathcal F_{k-1}}(Y_k-Y_{k-1})^2
=\mathbb E_{\mathcal F_{k-1}}
\frac{(X_k-X_{k-1})^2}{(A_k^X)^2}.
$$

Since $A_k^X$ is predictable, $A_k^X\in\mathcal F_{k-1}$, so it can be pulled out:
$$
=\frac{\mathbb E_{\mathcal F_{k-1}}(X_k-X_{k-1})^2}{(A_k^X)^2}.
\tag{1}
$$

Now observe directly from the definition of $A_k^X$:
$$
A_k^X-A_{k-1}^X
=\mathbb E_{\mathcal F_{k-1}}(X_k-X_{k-1})^2.
$$

Substituting into (1) gives
$$
\mathbb E_{\mathcal F_{k-1}}(Y_k-Y_{k-1})^2
=\frac{A_k^X-A_{k-1}^X}{(A_k^X)^2}.
$$

To bound this, use only monotonicity:
- $A_k^X\ge A_1^X=C$,
- $A_k^X-A_{k-1}^X\le A_k^X$.

Hence
$$
\frac{A_k^X-A_{k-1}^X}{(A_k^X)^2}
\le \frac{1}{A_k^X}
\le \frac{1}{C}.
$$
∎

#### Conclusion
Each normalized increment of $Y$ has uniformly bounded conditional variance.

#### Key Takeaways
- Predictability lets $A_k^X$ exit conditional expectations.
- $A_k^X-A_{k-1}^X$ is *exactly* the conditional variance increment.
- Bounding uses only monotonicity, no summability yet.

---

### a(ii)

#### Claim
$\{Y_k,\mathcal F_k\}$ is a martingale and $\mathbb E(Y_k^2)<\infty$ for all $k$.

#### Proof

**Adaptedness.**  
Each summand $(X_k-X_{k-1})/A_k^X$ is $\mathcal F_k$-measurable, hence $Y_n\in\mathcal F_n$.

**Integrability.**  
From part (a)(i),
$$
\mathbb E(\Delta Y_k^2)
=\mathbb E\big[\mathbb E_{\mathcal F_{k-1}}(\Delta Y_k^2)\big]
\le \frac{1}{C}.
$$
By Cauchy–Schwarz,
$$
\mathbb E|\Delta Y_k|
\le \sqrt{\mathbb E(\Delta Y_k^2)}<\infty.
$$
Since $Y_n=\sum_{k=1}^n\Delta Y_k$ is a *finite* sum,
$$
\mathbb E|Y_n|<\infty.
$$

**Martingale property.**
$$
\mathbb E_{\mathcal F_{n-1}}(Y_n-Y_{n-1})
=\frac{1}{A_n^X}
\bigl(\mathbb E_{\mathcal F_{n-1}}X_n-X_{n-1}\bigr)
=0.
$$

**Second moment.**  
Martingale differences are orthogonal:
$$
\mathbb E(\Delta Y_i\Delta Y_j)=0\quad(i<j).
$$
Thus
$$
\mathbb E(Y_n^2)
=\sum_{k=1}^n \mathbb E(\Delta Y_k^2)<\infty.
$$
∎

#### Conclusion
$Y$ is an $L^2$ martingale.

#### Key Takeaways
- Martingale integrability is **for each fixed $n$**, not uniform.
- Orthogonality of increments simplifies $L^2$ calculations.

---

### b(i)

#### Claim
$$
A_n^Y
=\sum_{k=1}^n \frac{A_k^X-A_{k-1}^X}{(A_k^X)^2}
\le C^{-1}\quad\text{a.s.}
$$

#### Proof

From part (a)(i),
$$
A_n^Y
=\sum_{k=1}^n \frac{A_k^X-A_{k-1}^X}{(A_k^X)^2}.
$$

Now **pause and reinterpret the sum**.

- $A_k^X-A_{k-1}^X$ plays the role of a *measure increment*.
- $(A_k^X)^{-2}$ is a decreasing function of the current quadratic variation level.

Let $f(x)=x^{-2}$ for $x\ge C$.  
Then $f$ is positive and decreasing, and the sum can be read as a **right-endpoint Riemann–Stieltjes sum**:
$$
\sum_{k=1}^n f(A_k^X)\,(A_k^X-A_{k-1}^X).
$$

For a decreasing function, right-endpoint rectangles lie **below** the curve, so
$$
\sum_{k=1}^n \frac{A_k^X-A_{k-1}^X}{(A_k^X)^2}
\le \int_{A_1^X}^\infty x^{-2}\,dx
=\int_C^\infty x^{-2}\,dx
=C^{-1}.
$$
∎

#### Conclusion
The quadratic variation of $Y$ is uniformly bounded.

#### Key Takeaways
- Quadratic variation behaves like a **Stieltjes measure**.
- Right-endpoint + decreasing function ⇒ sum ≤ integral.
- This pattern appears repeatedly in martingale limit theorems.

---

### b(ii)

#### Claim
$$
\sup_n \mathbb E(Y_n^2)<\infty
\quad\text{and}\quad
Y_n\to Y_\infty \text{ a.s.}
$$

#### Proof

From orthogonality,
$$
\mathbb E(Y_n^2)
=\mathbb E(A_n^Y)
\le C^{-1}.
$$
Hence $\sup_n\mathbb E(Y_n^2)<\infty$.
By the martingale convergence theorem, $Y_n$ converges almost surely (and in $L^2$).
∎

#### Conclusion
$Y_n$ converges a.s. to a finite limit.

#### Key Takeaways
- $L^2$-bounded martingales converge a.s.
- Bounded quadratic variation ⇒ bounded second moments.

---

### c

#### Claim
If $A_n^X\to\infty$ a.s., then
$$
\frac{X_n}{A_n^X}\to 0 \quad\text{a.s.}
$$

#### Proof

Write $D_k=X_k-X_{k-1}$, a martingale difference sequence.
From part (b), the series
$$
\sum_{k=1}^\infty \frac{D_k}{A_k^X}
$$
converges almost surely.
By **Kronecker’s Lemma (martingale difference version)**,
$$
\frac{1}{A_n^X}\sum_{k=1}^n D_k \to 0 \quad\text{a.s.}
$$
Since $\sum_{k=1}^n D_k=X_n-X_0$, the claim follows.
∎

#### Conclusion
Diverging quadratic variation forces sublinear martingale growth.

#### Key Takeaways
- Kronecker converts convergence of weighted increments into normalized limits.
- This is the bridge from $Y_n$ back to $X_n$.

---
