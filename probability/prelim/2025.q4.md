# 2025-Q4 – Truncation, Tail Control, and a Strong Law with Variable Normalization

This entry uses the official exam text and your original solution notes.   

---

## Problem 4 (verbatim)

Let $X$ be a random variable that satisfies  
$$
\sum_{k=1}^{\infty} P(|X| \ge a_k) < \infty,
$$  
where $\{a_k\}_{k\ge1}$ is a non negative and non decreasing sequence of real numbers that satisfies

1. $a_k \to \infty$ as $k\to\infty$.
2. There exists $C<\infty$ so that for each $n\ge1$,
   $$
   \sum_{k=n}^{\infty} a_k^{-2} \le C n a_n^{-2}.
   $$

Let $\{X,X_k\}_{k\ge1}$ be an i.i.d. sequence of random variables, and denote  
$$
Y_k = X_k \cdot 1_{\{|X_k|\le a_k\}},\qquad k\ge1.
$$  
Prove the following.

---

### (a)(i)  
$$
\frac{\sum_{k=1}^n X_k \cdot 1_{\{|X_k|>a_k\}}}{a_n} \xrightarrow[n\to\infty]{\text{a.s.}} 0.
$$

### (a)(ii)  
If  
$$
\frac{\sum_{k=1}^n (Y_k - E Y_k)}{a_n} \xrightarrow[n\to\infty]{\text{a.s.}} 0,
$$  
then  
$$
\frac{\sum_{k=1}^n X_k - E Y_k}{a_n} \xrightarrow[n\to\infty]{\text{a.s.}} 0.
$$

---

### (b)(i)  
Let $a_0=0$. Prove
$$
\sum_{n=1}^{\infty} a_n^{-2} E(Y_n^2)
  = \sum_{k=1}^{\infty} E\!\left(X^2 1_{\{a_{k-1}<|X|\le a_k\}}\right)
      \left(\sum_{n=k}^{\infty} a_n^{-2}\right).
$$

### (b)(ii)  
Use the result of part (i) to prove that
$$
\sum_{n=1}^{\infty} a_n^{-2} E(Y_n^2)
\le C \sum_{k=1}^{\infty} k\, P(a_{k-1}<|X|\le a_k).
$$

---

### (c)(i)  
Show that
$$
\sum_{k=1}^{\infty} \frac{Y_k - E(Y_k)}{a_k}
\quad\text{converges a.s.}
$$

Hint:  
$$
\sum_{k=1}^{\infty} k\,P(a_{k-1}<|X|\le a_k)
 = \sum_{k=0}^{\infty} P(|X|>a_k)
$$  
by summation by parts.

### (c)(ii)  
$$
\frac{\sum_{k=1}^{n} X_k - E(Y_k)}{a_n} \xrightarrow[n\to\infty]{\text{a.s.}} 0.
$$

---

# Solutions in Claim–Proof–Conclusion Format

---

## (a)(i) Tail part is negligible

### **Claim**  
$$
\frac{1}{a_n}\sum_{k=1}^n X_k 1_{\{|X_k|>a_k\}} \xrightarrow[n\to\infty]{\text{a.s.}} 0.
$$

### **Proof**

Define the events
$$
A_k = \{|X_k|\ge a_k\}.
$$

Since $X_k$ has the same distribution as $X$,
$$
P(A_k) = P(|X_k|\ge a_k) = P(|X|\ge a_k).
$$

The assumption gives
$$
\sum_{k=1}^\infty P(A_k) = \sum_{k=1}^\infty P(|X|\ge a_k) < \infty.
$$

By the first Borel Cantelli lemma,
$$
P(A_k\ \text{i.o.}) = 0.
$$  
So with probability one there exists a random index $K(\omega)$ such that for all $k\ge K(\omega)$,
$$
|X_k(\omega)| \le a_k.
$$  

On that event,
$$
\sum_{k=1}^n X_k(\omega) 1_{\{|X_k(\omega)|>a_k\}}
= \sum_{k=1}^{K(\omega)-1} X_k(\omega)1_{\{|X_k(\omega)|>a_k\}}
$$
for all $n\ge K(\omega)$. Hence the numerator stabilizes to a finite constant $C(\omega)$, while $a_n\to\infty$. Thus
$$
\frac{\sum_{k=1}^n X_k 1_{\{|X_k|>a_k\}}}{a_n}
\to \frac{C(\omega)}{\infty}=0
\quad\text{a.s.}
$$

### **Conclusion**  
The contribution from the large deviations $|X_k|>a_k$ is asymptotically negligible after normalization by $a_n$.

### **Key Takeaways**
- Recognize the tail indicator sum as a Borel Cantelli setup.  
- Once only finitely many “bad” events occur, the normalized partial sum must vanish since $a_n\to\infty$.

---

## (a)(ii) Passing from truncated to full sum

### **Claim**  
If  
$$
\frac{\sum_{k=1}^n (Y_k - EY_k)}{a_n} \to 0 \quad\text{a.s.},
$$  
then  
$$
\frac{\sum_{k=1}^n X_k - EY_k}{a_n} \to 0\quad\text{a.s.}
$$

### **Proof**

Decompose $X_k$ into truncated and tail parts:
$$
X_k = X_k 1_{\{|X_k|\le a_k\}} + X_k 1_{\{|X_k|>a_k\}}
= Y_k + X_k 1_{\{|X_k|>a_k\}}.
$$

Then
$$
\sum_{k=1}^n X_k - E Y_k
= \sum_{k=1}^n (Y_k -EY_k)
  + \sum_{k=1}^n X_k 1_{\{|X_k|>a_k\}}.
$$

Divide by $a_n$:
$$
\frac{\sum_{k=1}^n X_k - E Y_k}{a_n}
= \frac{\sum_{k=1}^n (Y_k -EY_k)}{a_n}
  + \frac{\sum_{k=1}^n X_k 1_{\{|X_k|>a_k\}}}{a_n}.
$$

By the assumption, the first term tends to zero almost surely.  
By part (a)(i), the second term also tends to zero almost surely.  
Therefore the sum tends to zero almost surely.

### **Conclusion**  
As soon as the truncated sum behaves well, the full sum with the same normalization also behaves well.

### **Key Takeaways**
- Break the full sum into truncated part plus tail part.  
- Use part (a)(i) to control the tails.  
- Linearity of expectation and algebraic decompositions are your main tools.

---

## (b)(i) Rearranging the double sum

### **Claim**  
With $a_0=0$,
$$
\sum_{n=1}^{\infty} a_n^{-2} E(Y_n^2)
= \sum_{k=1}^{\infty} E\!\left(X^2 1_{\{a_{k-1}<|X|\le a_k\}}\right)
     \left(\sum_{n=k}^{\infty} a_n^{-2}\right).
$$

### **Proof**

For each $n$,
$$
Y_n^2 = X_n^2 1_{\{|X_n|\le a_n\}}.
$$

Since $X_n\stackrel{d}{=} X$,
$$
E(Y_n^2)
= E(X^2 1_{\{|X|\le a_n\}}).
$$

Partition $\{|X|\le a_n\}$ into disjoint “annuli”:
$$
\{|X|\le a_n\} = \bigcup_{k=1}^{n} \{a_{k-1}<|X|\le a_k\}.
$$

Therefore
$$
E(Y_n^2)
= \sum_{k=1}^{n} E\!\left(X^2 1_{\{a_{k-1}<|X|\le a_k\}}\right).
$$

Now sum in $n$ with weight $a_n^{-2}$:
$$
\sum_{n=1}^{\infty} a_n^{-2} E(Y_n^2)
= \sum_{n=1}^{\infty} a_n^{-2} \sum_{k=1}^{n} E\!\left(X^2 1_{\{a_{k-1}<|X|\le a_k\}}\right).
$$

All terms are non negative, so Tonelli’s theorem allows us to swap sums:
$$
= \sum_{k=1}^{\infty} E\!\left(X^2 1_{\{a_{k-1}<|X|\le a_k\}}\right)
    \sum_{n=k}^{\infty} a_n^{-2}.
$$

### **Conclusion**  
We have expressed the global series as a sum over “shells” in $|X|$, each weighted by a tail sum of $a_n^{-2}$.

### **Key Takeaways**
- Use the decomposition $\{|X|\le a_n\} = \bigcup_{k\le n}\{a_{k-1}<|X|\le a_k\}$.  
- Non negative terms justify interchanging sums via Tonelli.  

---

## (b)(ii) Bounding by tail probabilities

### **Claim**  
$$
\sum_{n=1}^{\infty} a_n^{-2} E(Y_n^2)
\le C \sum_{k=1}^{\infty} k\, P(a_{k-1}<|X|\le a_k).
$$

### **Proof**

From part (b)(i),
$$
\sum_{n=1}^{\infty} a_n^{-2} E(Y_n^2)
= \sum_{k=1}^{\infty} E\!\left(X^2 1_{\{a_{k-1}<|X|\le a_k\}}\right)
   \left(\sum_{n=k}^{\infty} a_n^{-2}\right).
$$

By the condition on $(a_n)$,
$$
\sum_{n=k}^{\infty} a_n^{-2} \le C k a_k^{-2}.
$$

Also on $\{a_{k-1}<|X|\le a_k\}$ we have $|X|\le a_k$, so
$$
X^2 1_{\{a_{k-1}<|X|\le a_k\}}
\le a_k^2 1_{\{a_{k-1}<|X|\le a_k\}}.
$$

Hence
$$
E\!\left(X^2 1_{\{a_{k-1}<|X|\le a_k\}}\right)
\le a_k^2 P(a_{k-1}<|X|\le a_k).
$$

Combine:
$$
\sum_{n=1}^{\infty} a_n^{-2} E(Y_n^2)
\le \sum_{k=1}^{\infty}
   a_k^2 P(a_{k-1}<|X|\le a_k)\, C k a_k^{-2}
= C\sum_{k=1}^{\infty} k\, P(a_{k-1}<|X|\le a_k).
$$

### **Conclusion**  
The series of second moments of $Y_n/a_n$ is bounded by a weighted tail sum involving $P(a_{k-1}<|X|\le a_k)$.

### **Key Takeaways**
- Use the growth condition on $(a_n)$ to control $\sum_{n=k}^{\infty} a_n^{-2}$.  
- Bound $X^2$ by $a_k^2$ on the event where $|X|\le a_k$.  

---

## (c)(i) Almost sure convergence of the normalized series

### **Claim**  
The series  
$$
\sum_{k=1}^{\infty} \frac{Y_k - E(Y_k)}{a_k}
$$  
converges almost surely.

### **Proof**

Let
$$
Z_k := \frac{Y_k - E(Y_k)}{a_k},\qquad k\ge1.
$$

The $Y_k$ are independent (functions of independent $X_k$), hence the $Z_k$ are independent and centered:
$$
E(Z_k)=0.
$$

Compute their variances:
$$
\mathrm{Var}(Z_k) = a_k^{-2}\,\mathrm{Var}(Y_k)
\le a_k^{-2} E(Y_k^2).
$$

From part (b)(ii),
$$
\sum_{k=1}^{\infty} a_k^{-2} E(Y_k^2)
\le C \sum_{k=1}^{\infty} k P(a_{k-1}<|X|\le a_k).
$$

By the hint and the assumption $\sum_{k=1}^{\infty}P(|X|\ge a_k)<\infty$, one checks
$$
\sum_{k=1}^{\infty} k P(a_{k-1}<|X|\le a_k)
= \sum_{k=0}^{\infty} P(|X|>a_k) < \infty.
$$

Thus
$$
\sum_{k=1}^{\infty} \mathrm{Var}(Z_k)
\le \sum_{k=1}^{\infty} a_k^{-2} E(Y_k^2) < \infty.
$$

By the standard theorem for series of independent centered random variables  
(if $\sum \mathrm{Var}(Z_k)<\infty$, then $\sum Z_k$ converges a.s.),  
we get that  
$$
\sum_{k=1}^{\infty} Z_k
= \sum_{k=1}^{\infty} \frac{Y_k - E(Y_k)}{a_k}
$$
converges almost surely.

### **Conclusion**  
The series of normalized centered truncations converges almost surely.

### **Key Takeaways**
- Use the bound on $\sum a_k^{-2}E(Y_k^2)$ from (b)(ii).  
- The “variance test” for independence is a powerful tool for series of random variables.  

---

## (c)(ii) Final strong law type result

### **Claim**  
$$
\frac{\sum_{k=1}^{n} X_k - E(Y_k)}{a_n}
\xrightarrow[n\to\infty]{\text{a.s.}} 0.
$$

### **Proof**

From part (c)(i), the series
$$
\sum_{k=1}^{\infty} \frac{Y_k - E(Y_k)}{a_k}
$$
converges a.s.  
Let
$$
T_n := \sum_{k=1}^{n} \frac{Y_k - E(Y_k)}{a_k},
\quad\text{so } T_n\to T \text{ a.s. for some finite }T.
$$

Note that
$$
Y_k - E(Y_k) = a_k (T_k - T_{k-1}),\quad T_0=0.
$$

Let
$$
S_n := \sum_{k=1}^n (Y_k - E(Y_k)).
$$

Then
$$
S_n = \sum_{k=1}^n a_k (T_k - T_{k-1}).
$$

Apply discrete summation by parts:
$$
\sum_{k=1}^n a_k (T_k - T_{k-1})
= a_n T_n - \sum_{k=0}^{n-1} T_k (a_{k+1}-a_k).
$$

Divide by $a_n$:
$$
\frac{S_n}{a_n} = T_n - \frac{1}{a_n}\sum_{k=0}^{n-1} T_k (a_{k+1}-a_k).
$$

Since $T_n\to T$ a.s., the sequence $(T_k)$ is bounded a.s. by some random $M(\omega)$.  
The weights
$$
w_{k,n} := \frac{a_{k+1}-a_k}{a_n},\quad k=0,\dots,n-1
$$
are non negative and sum to $1$:
$$
\sum_{k=0}^{n-1} w_{k,n}
= \frac{a_n - a_0}{a_n} = 1.
$$

Hence the second term is a weighted average:
$$
\frac{1}{a_n}\sum_{k=0}^{n-1} T_k (a_{k+1}-a_k)
= \sum_{k=0}^{n-1} T_k w_{k,n}.
$$

Because $T_k\to T$ and $(w_{k,n})$ form a probability distribution on $\{0,\dots,n-1\}$, this weighted average also tends to $T$. Therefore a.s.
$$
\frac{S_n}{a_n}
= T_n - \sum_{k=0}^{n-1} T_k w_{k,n}
\to T - T = 0.
$$

So
$$
\frac{\sum_{k=1}^{n} (Y_k - E(Y_k))}{a_n}
\to 0 \quad\text{a.s.}
$$

Finally, apply part (a)(ii): since we have shown exactly the condition required there, it follows that
$$
\frac{\sum_{k=1}^{n} X_k - E(Y_k)}{a_n}
\xrightarrow[n\to\infty]{\text{a.s.}} 0.
$$

### **Conclusion**  
The properly normalized sum of the original $X_k$ behaves like its truncated expectation:
$$
\frac{\sum_{k=1}^{n} X_k - E(Y_k)}{a_n}\to 0\quad\text{a.s.}
$$

### **Key Takeaways**
- Convergence of the series $\sum (Y_k - E(Y_k))/a_k$ plus monotonicity of $a_k$ yields $S_n/a_n\to0$ via summation by parts.  
- Part (a)(ii) then bridges from truncated variables back to the original $X_k$.  
- This is a “generalized law of large numbers” with non standard normalization $a_n$ chosen to control the tails.
