# 2022-Q4 — Upper Bounds for Brownian Motion Maxima

[2022 Probability Prelim Exam (PDF)](https://stt.natsci.msu.edu/_assets/files/graduate-programs/Prelim%20Prob%20Exam%202022.pdf)


## Theorems & Tools Used (Cheat-Sheet Index)
- **Markov’s Inequality**:  
  $\mathbb P(X>a)\le \mathbb E[X]/a$ for $X\ge 0$.
- **Chernoff (Exponential Markov) Bound**:  
  Apply Markov to $e^{tX}$, then optimize over $t>0$.
- **Normal MGF**:  
  If $Z\sim N(0,1)$, then $\mathbb E[e^{tZ}]=e^{t^2/2}$.
- **Reflection Principle (Brownian Motion)**:  
  $\mathbb P(\max_{s\le t}B(s)>x)=2\mathbb P(B(t)>x)$.
- **Brownian Scaling**:  
  $B(t)\stackrel{d}{=}\sqrt t\,Z$, $Z\sim N(0,1)$.
- **Borel–Cantelli Lemma (I)**:  
  If $\sum_n \mathbb P(A_n)<\infty$, then $\mathbb P(A_n\ \mathrm{i.o.})=0$.
- **$p$-Series Test**:  
  $\sum_{n=1}^\infty n^{-p}<\infty$ iff $p>1$.
- **Monotonicity of $\log\log x$** (eventually):  
  Used to compare normalizing terms on intervals.
- **Limit Rules for Limsup**:  
  $\limsup (a_nb_n)\le (\limsup a_n)(\limsup b_n)$ for $a_n,b_n\ge 0$.

---

## Verbatim Question (from Prob_F22)

Let $Z \sim N(0,1)$.

**(a)(i)** Prove that
$$
\mathbb{P}(Z>x) \le e^{-x^2/2}, \qquad x>0.
$$

**(a)(ii)** Let $(B(s))_{s\ge0}$ be standard Brownian motion. Prove that
$$
\mathbb{P}\!\left(\max_{0\le s\le t} B(s) > x\right)
\le 2\exp\!\left(-\frac{x^2}{2t}\right),
\qquad x>0,\ t>0.
$$

For the rest of the problem, let $\alpha>1$ and define $t_n=\alpha^n$, $n=1,2,\dots$.

**(b)** Show that
$$
\limsup_{n\to\infty}
\frac{\max_{0\le s\le t_n} B(s)}
{\sqrt{2t_{n+1}\log\log(t_{n+1})}}
\le 1
\quad\text{a.s.}
$$

**(c)** Show that
$$
\limsup_{n\to\infty}
\max_{t_{n-1}\le s\le t_n}
\frac{B(s)}{\sqrt{2s\log\log(s)}}
\le \alpha
\quad\text{a.s.}
$$

---

## (a)(i)

### Claim
For $Z\sim N(0,1)$ and $x>0$,
$$
\mathbb{P}(Z>x)\le e^{-x^2/2}.
$$

### Proof
For any $t>0$,
$$
\mathbb{P}(Z>x)=\mathbb{P}(e^{tZ}>e^{tx})
\le \frac{\mathbb{E}[e^{tZ}]}{e^{tx}}
= \exp\!\left(\frac{t^2}{2}-tx\right),
$$
by **Markov’s inequality** applied to $e^{tZ}$ (Chernoff bound) and the
normal MGF.
Minimizing the exponent over $t>0$ gives $t=x$, yielding the result.

### Conclusion
The standard normal upper tail is sub-Gaussian with variance proxy $1$.

### Key Takeaway
Chernoff bounds plus MGF optimization give sharp Gaussian tail estimates.

---

## (a)(ii)

### Claim
For $x>0$ and $t>0$,
$$
\mathbb{P}\!\left(\max_{0\le s\le t} B(s)>x\right)
\le 2\exp\!\left(-\frac{x^2}{2t}\right).
$$

### Proof
By the **reflection principle**,
$$
\mathbb{P}\!\left(\max_{0\le s\le t} B(s)>x\right)
=2\,\mathbb{P}(B(t)>x).
$$
Using **Brownian scaling**, $B(t)=\sqrt t\,Z$ with $Z\sim N(0,1)$, and
applying part (a)(i) gives the bound.

### Conclusion
Finite-time Brownian maxima inherit Gaussian tail decay.

### Key Takeaway
Reflection principle + scaling reduces pathwise bounds to one-dimensional
Gaussian tails.

---

## (b)

### Claim
If $\alpha>1$ and $t_n=\alpha^n$,
$$
\limsup_{n\to\infty}
\frac{\max_{0\le s\le t_n} B(s)}
{\sqrt{2t_{n+1}\log\log(t_{n+1})}}
\le 1
\quad\text{a.s.}
$$

### Proof
Define
$$
A_n=\left\{M_{t_n}>\sqrt{2t_{n+1}\log\log(t_{n+1})}\right\}.
$$
Using (a)(ii),
$$
\mathbb{P}(A_n)\le
2\exp\!\left(-\alpha\log\log(t_{n+1})\right)
\le C(n+1)^{-\alpha}.
$$
Since $\alpha>1$, the series $\sum_n \mathbb{P}(A_n)$ converges by the
**$p$-series test**. By **Borel–Cantelli (I)**, $A_n$ occurs only finitely
often, giving the almost-sure bound.

### Conclusion
On a geometric time grid, Brownian maxima are eventually controlled by the
LIL scale.

### Key Takeaway
Borel–Cantelli turns summable tail bounds into almost-sure eventual control.

---

## (c)

### Claim
$$
\limsup_{n\to\infty}
\max_{t_{n-1}\le s\le t_n}
\frac{B(s)}{\sqrt{2s\log\log(s)}}
\le \alpha
\quad\text{a.s.}
$$

### Proof
By monotonicity of $\sqrt{2s\log\log s}$,
$$
\max_{t_{n-1}\le s\le t_n}
\frac{B(s)}{\sqrt{2s\log\log(s)}}
\le
\frac{M_{t_n}}{\sqrt{2t_{n-1}\log\log(t_{n-1})}}.
$$
Use the eventual bound from part (b) and compute the limit of the ratio of
normalizing constants to obtain $\alpha$.

### Conclusion
Blockwise Brownian fluctuations obey an LIL upper bound with constant
$\alpha$.

### Key Takeaway
Discrete-time almost-sure bounds extend to continuous intervals via
deterministic scale comparisons.

---

## Summary of Key Takeaways
- Markov + Chernoff + MGFs control Gaussian tails.
- Reflection principle is essential for Brownian maxima.
- Borel–Cantelli converts summable probabilities into a.s. bounds.
- Geometric discretization ($t_n=\alpha^n$) avoids critical divergence.
- $p$-series convergence ($p>1$) is a recurring tool in LIL proofs.
- The $(1+\varepsilon)$-trick is only needed at critical scaling.
