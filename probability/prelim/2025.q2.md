# 2025-Q2 – Symmetric Variables, Conditional Expectation, and Quadratic Variation

2025 Probability Prelim Exam (PDF) (link pending)

## Problem 2 (verbatim)

### (a)(i)  
Let $X$ be a symmetric random variable (i.e., $X \stackrel{d}{=} -X$) with $P(X=0)=0$.  
Prove that  
$$
X = \varepsilon \cdot |X| \quad \text{a.s.},
$$  
where $\varepsilon$ and $|X|$ are independent, and  
$$
P(\varepsilon = 1)=P(\varepsilon=-1)=\tfrac12.
$$

---

### (a)(ii)  
Let $X,Y$ be symmetric, independent, and square-integrable random variables with  
$P(X=0)=P(Y=0)=0$.  
Let  
$$
\mathcal H = \sigma(|X|,|Y|)=\sigma(X^2,Y^2).
$$  
Prove that  
$$
E_{\mathcal H}(XY)=0.
$$  
Also calculate $E_{\mathcal H}[(X+Y)^2]$.

---

### (b)  
Let $\{B_t : t\ge0\}$ be standard Brownian motion.  
Let  
$$
Q_n = \sum_{k=1}^n D_{k,n}^2,\qquad D_{k,n}=B_{t_{k,n}}-B_{t_{k-1,n}},
$$  
where  
$0=t_{0,n}<t_{1,n}<\cdots<t_{n,n}=1$  
and  
$\Delta_{k,n}=t_{k,n}-t_{k-1,n}$.

(i) Compute  
$$
E(Q_n)\qquad\text{and}\qquad \mathrm{Var}(Q_n).
$$

(ii) Let $\Delta_n=\max_{1\le k\le n}\Delta_{k,n}$.  
Prove that if $\Delta_n\to0$ then $Q_n\to1$ in probability.

---

### (c)  
Assume that the partitions are nested:  
$$
\{t_{k,n}\}_{k=0}^n \subset \{t_{k,n+1}\}_{k=0}^{n+1}.
$$

Let  
$$
\mathcal H_n = \sigma\left(\bigcup_{m=n}^\infty \{D_{k,m}^2:1\le k\le m\}\right).
$$  
This is a decreasing filtration.

(i) Compute $E_{\mathcal H_{n+1}}(Q_n)$ and identify the type of process $\{Q_n,\mathcal H_n\}$.

(ii) Prove that if $\Delta_n\to0$ then $Q_n\to1$ almost surely.

---

# Survival Guide Solution

## (a)(i) Symmetric RVs decompose into sign and magnitude

### **Claim**  
For symmetric $X$ with $P(X=0)=0$, the representation  
$$
X=\varepsilon |X|,\qquad P(\varepsilon=1)=P(\varepsilon=-1)=\tfrac12,
$$  
holds a.s., and $\varepsilon$ is independent of $|X|$.

### **Proof**

1. **Define the sign variable.**  
Let  
$$
\varepsilon(\omega)=\begin{cases}
+1,& X(\omega)>0
-1,& X(\omega)<0
\end{cases}.
$$  
Since $P(X=0)=0$, $\varepsilon$ is well-defined a.s.  
Then trivially  
$$
X=\varepsilon |X|.
$$

2. **Show that $\varepsilon$ is Rademacher.**  
Symmetry means  
$$
P(X>0) = P(-X>0) = P(X<0).
$$  
Because $P(X\ne0)=1$,  
$$
P(\varepsilon=1)=P(X>0)=P(X<0)=P(\varepsilon=-1)=\tfrac12.
$$

3. **Show independence of $\varepsilon$ and $|X|$.**  
For any Borel set $A\subset[0,\infty)$,  
$$
P(\varepsilon=1, |X|\in A)
= P(X>0, |X|\in A)
= P(X\in A),
$$
and by symmetry,  
$$
P(\varepsilon=-1, |X|\in A)
= P(X<0, |X|\in A)
= P(X\in -A)=P(X\in A).
$$
Also by symmetry,
$$
P(\vert X\vert\in A)=P(X\in A) + P(X\in-A)=2P(X\in A) \text{ implies } P(X\in A)=\frac{1}{2}P(\vert X\vert\in A) 
$$

Thus  
$$
P(\varepsilon=\pm1, |X|\in A)
= \tfrac12\, P(|X|\in A)
= P(\varepsilon=\pm1)\, P(|X|\in A).
$$

### **Conclusion**

There exists an independent Rademacher $\varepsilon$ such that  
$$X=\varepsilon |X| \quad \text{a.s.}.$$

### **Key Takeaways**
- Symmetry enables decomposition into sign and magnitude.  
- Independence follows from splitting probability into positive/negative branches.  
- “Almost sure” allows breaking ties at $X=0$ on a null set.

---

## (a)(ii) Conditional expectations given magnitudes

### **Claim**  
For symmetric, independent $X,Y$ with $P(X=0)=P(Y=0)=0$,  
$$
E_{\mathcal H}(XY)=0,
$$  
and  
$$
E_{\mathcal H}[(X+Y)^2]=X^2+Y^2.
$$

### **Proof**

1. **Decompose each variable.**  
From (a)(i),  
$$
X = \varepsilon_X |X|,\qquad Y=\varepsilon_Y |Y|,
$$  
where $\varepsilon_X,\varepsilon_Y$ are independent Rademacher variables and are independent of $\mathcal H$.

2. **Compute the cross term.**  
$$
XY = \varepsilon_X\varepsilon_Y |X||Y|.
$$  
Conditional on $\mathcal H$, the magnitudes are known constants, while the signs remain independent with mean zero:  
$$
E(\varepsilon_X\varepsilon_Y\mid\mathcal H)=E(\varepsilon_X)E(\varepsilon_Y)=0.
$$  
Thus  
$$
E_{\mathcal H}(XY)=|X||Y|\cdot 0=0.
$$

3. **Compute $E_{\mathcal H}[(X+Y)^2]$.**  
Expand:  
$$
(X+Y)^2 = X^2 + Y^2 + 2XY.
$$  
Taking conditional expectation,  
$$
E_{\mathcal H}(X^2)=X^2,\qquad 
E_{\mathcal H}(Y^2)=Y^2,
$$
and from step 2,  
$$
E_{\mathcal H}(XY)=0.
$$

Hence  
$$
E_{\mathcal H}[(X+Y)^2]=X^2 + Y^2.
$$

### **Conclusion**  
Conditioning on magnitudes removes sign-information, killing all odd or mixed terms.

### **Key Takeaways**
- When conditioning on magnitudes (an even function), **all odd symmetric parts vanish**.  
- Independence of signs is the crucial structural simplification.  
- Linearize and expand before conditioning.

---

## (b)(i) Expectation and variance of quadratic variation approximations

Recall  
$$Q_n=\sum_{k=1}^n D_{k,n}^2,\qquad D_{k,n}\sim N(0,\Delta_{k,n}).$$

### **Claim**  
$$
E(Q_n)=\sum_{k=1}^n \Delta_{k,n},\qquad
\mathrm{Var}(Q_n)=2\sum_{k=1}^n \Delta_{k,n}^2.
$$

### **Proof**

1. **Expectation.**  
For $D\sim N(0,\Delta)$,  
$$
E(D^2)=\Delta.
$$  
Since increments are independent,  
$$
E(Q_n)=\sum_{k=1}^n E(D_{k,n}^2)=\sum_{k=1}^n\Delta_{k,n}=1.
$$

2. **Variance.**  
For $D\sim N(0,\Delta)$,  
$$
\mathrm{Var}(D^2)=2\Delta^2.
$$  
Independence gives  
$$
\mathrm{Var}(Q_n)=\sum_{k=1}^n 2\Delta_{k,n}^2.
$$

### **Conclusion**  
$E(Q_n)=1$ for any partition; variance is the sum of squared mesh lengths.

### **Key Takeaways**
- $D^2$ for a normal increment has variance $2\Delta^2$.  
- These sums approximate quadratic variation.  
- Brownian increments are independent and Gaussian.

---

## (b)(ii) Convergence in probability

### **Claim**  
If $\Delta_n\to0$, then $Q_n\to1$ in probability.

### **Proof**  
From (i),
$$
\mathrm{Var}(Q_n)=2\sum_{k=1}^n \Delta_{k,n}^2
\le 2\Delta_n \sum_{k=1}^n \Delta_{k,n}
=2\Delta_n.
$$

Apply Chebyshev:
$$
P(|Q_n-1|>\varepsilon)
\le \frac{\mathrm{Var}(Q_n)}{\varepsilon^2}
\le \frac{2\Delta_n}{\varepsilon^2}
\to 0.
$$

### **Conclusion**  
Mesh size $\Delta_n\to0$ implies $Q_n\to1$ in probability.

### **Key Takeaways**
- Controlling variance is the key step for probability convergence.  
- $\sum\Delta_{k,n}=1$ is fixed, so only $\Delta_n$ matters.

---

## (c)(i) Reverse martingale structure

Adding a point splits exactly one interval:  
$$
(t_{k_*,n},t_{k_*+1,n}]
=
(t_{k_*,n+1},t_{k_*+1,n+1}]
\cup
(t_{k_*+1,n+1},t_{k_*+2,n+1}].
$$

Thus  
$$
D_{k_*,n}^2
= D_{k_*,n+1}^2 + D_{k_*+1,n+1}^2,
$$  
and all other increments are unchanged.

### **Claim**  
$$
E_{\mathcal H_{n+1}}(Q_n)=Q_{n+1}.
$$

### **Proof**  
Write  
$$
Q_n=\sum_{k\ne k_*} D_{k,n}^2 + D_{k_*,n}^2
= \sum_{k\ne k_*} D_{k,n+1}^2 + (D_{k_*,n+1}^2 + D_{k_*+1,n+1}^2)
=Q_{n+1}.
$$

Since $D_{k,m}^2\in \mathcal H_m$ and $\mathcal H_{n+1}\subset\mathcal H_n$, all terms are measurable with respect to $\mathcal H_{n+1}$.  
Therefore the conditional expectation is deterministic and equals $Q_{n+1}$.

Hence  
$$
E(Q_n\mid \mathcal H_{n+1}) = Q_{n+1}.
$$

### **Conclusion**  
$\{Q_n,\mathcal H_n\}$ is a **reverse martingale**.

### **Key Takeaways**
- Nested partitions create reverse filtrations.  
- The quadratic variation approximation decreases in terms of $\mathcal H_n$–information.  
- Reverse martingales converge almost surely (Doob’s reverse martingale theorem).

---

## (c)(ii) Almost sure convergence of $Q_n$

### **Claim**  
If $\Delta_n\to0$ then $Q_n\to1$ a.s.

### **Proof**  
From (c)(i), $Q_n$ is a reverse martingale with  
$$
E(Q_n)=1,\qquad Q_n\ge0.
$$

By the **reverse martingale convergence theorem**,  
$$
Q_n \to Q_\infty \quad\text{a.s.},
$$  
for some integrable $Q_\infty$.

But from (b)(ii),  
$$
Q_n \to 1 \quad\text{in probability}.
$$

A reverse martingale limit is unique, so convergence in probability to $1$ forces  
$$
Q_\infty = 1 \quad\text{a.s.}
$$

Thus $Q_n\to1$ almost surely.

### **Conclusion**  
Quadratic variation approximations converge almost surely to $1$ whenever the mesh size goes to zero.

### **Key Takeaways**
- Reverse martingale convergence + convergence in probability pins down the a.s. limit.  
- Quadratic variation of Brownian motion on [0,1] is literally $1$.  
- This is the foundational theorem behind stochastic calculus.

