# 2023-Q2 – Equivalences and Counterexamples in Modes of Convergence

[2023 Probability Prelim Exam (PDF)](https://stt.natsci.msu.edu/_assets/files/graduate-programs/Prelim%20Prob%20Exam%202023.pdf)

## **Problem Statement**  (verbatim from prelim)  
:contentReference[oaicite:2]{index=2}  

Let $\{X_k\}$, $k = 1,2,\dots$ be a sequence of random variables. In this problem all
convergences are as $k \to \infty$.

**(a)** Prove that $X_k \to 0$ in probability **if and only if** $X_k \to 0$ in distribution.

**(b)**  
(i) Prove that if $X_k \to 0$ a.s. then  
$$
P\!\left(\bigcup_{m=k}^{\infty}\{|X_m| > \varepsilon\}\right)\to 0
\quad \text{for each } \varepsilon>0.
$$

(ii) Show how to conclude from (i) that convergence a.s. implies convergence in probability.

**(c)** Assume that $\{X_k\}$ are independent,  
$$
P(|X_k| > 1)=\frac{1}{k},  
\qquad  
P(|X_k| \le 1/\sqrt{k}) \to 1.
$$  

Show that $X_k \to 0$ in probability, but $X_k \to 0$ a.s. is **false**.

---

# **Solution**

## **(a) Convergence in probability iff convergence in distribution**  
:contentReference[oaicite:3]{index=3}  

### **Claim.**  
If $X_k \to 0$ in probability, then $X_k \to 0$ in distribution, and conversely.

### **Proof.**

**($\Rightarrow$)** Assume $X_k \to 0$ in probability.  
Let $F_k$ denote the cdf of $X_k$, and let $F$ be the cdf of the constant $0$.  
We must show that $F_k(x) \to F(x)$ for every continuity point of $F$.

Since $F(x) = \mathbf{1}\{x \ge 0\}$, the only point of discontinuity is $x = 0$.  
Thus we only check $x < 0$ and $x > 0$.

- **Case 1: $x < 0$.**  
  Pick $\varepsilon = |x| > 0$. Then
  $$
  F_k(x) = P(X_k \le x) \le P(|X_k|>\varepsilon).
  $$
  Since $X_k \to 0$ in probability, the RHS goes to $0$.  
  Also, $F(x)=0$, so $F_k(x)\to 0$.

- **Case 2: $x > 0$.**  
  Pick $\varepsilon = x$. Then
  $$
  P(X_k \le x) \ge 1 - P(|X_k| > x).
  $$
  The probability $P(|X_k| > x) \to 0$, so $F_k(x) \to 1 = F(x)$.

Thus $F_k(x)\to F(x)$ for all continuity points.

---

**($\Leftarrow$)**  
To show: convergence in distribution to $0$ implies convergence in probability to $0$.

For any $\varepsilon>0$, consider
$$
P(|X_k|>\varepsilon) = P(X_k<-\varepsilon) + P(X_k>\varepsilon).
$$
Let $x_1=-\varepsilon$ and $x_2=\varepsilon$. These are continuity points of $F$, so  
$$F_k(x_1)\to 0, \qquad F_k(x_2)\to 1.$$
Hence
$$
P(|X_k|>\varepsilon)
= F_k(-\varepsilon)+ (1-F_k(\varepsilon))
\to 0+0 = 0.
$$

### **Conclusion.**  
$X_k \to 0$ in probability if and only if $X_k \to 0$ in distribution.

---

## **(b)(i) If $X_k\to0$ a.s., then tail events vanish**

### **Claim.**  
If $X_k \to 0$ a.s., then for each $\varepsilon > 0$,  
$$
P\left(\bigcup_{m=k}^{\infty} \{|X_m| > \varepsilon\}\right) \longrightarrow 0.
$$

### **Proof.**

Fix $\varepsilon>0$ and define the tail events
$$
A_k = \bigcup_{m=k}^{\infty}\{|X_m|>\varepsilon\}.
$$

As $k$ increases, each $A_k$ loses elements, so $\{A_k\}$ is a **decreasing** sequence:
$$
A_{k+1}\subseteq A_k.
$$

Since $X_k\to0$ a.s., for almost every $\omega$ there exists some $K(\omega)$ such that  
$|X_m(\omega)|\le \varepsilon$ for all $m\ge K(\omega)$.  
Thus for almost all $\omega$,
$$
\omega\notin \bigcap_{k=1}^\infty A_k.
$$

Hence  
$$
P\!\left(\bigcap_{k=1}^\infty A_k \right)=0.
$$

Using continuity from above for decreasing events:
$$
P(A_k)\downarrow P\!\left(\bigcap_{k=1}^\infty A_k\right)=0.
$$

### **Conclusion.**  
The probability that the sequence ever exceeds $\varepsilon$ after time $k$ tends to $0$.

---

## **(b)(ii) Almost sure convergence implies convergence in probability**

### **Claim.**  
If $X_k\to0$ a.s., then $X_k\to0$ in probability.

### **Proof.**

From part (i), for any $\varepsilon>0$,
$$
P\left(\bigcup_{m=k}^{\infty}|X_m|>\varepsilon\right)\to0.
$$
But
$$
\{|X_k|>\varepsilon\} \subseteq \bigcup_{m=k}^{\infty}|X_m|>\varepsilon,
$$
so
$$
P(|X_k|>\varepsilon) \le P\left(\bigcup_{m=k}^{\infty}|X_m|>\varepsilon\right)\to0.
$$

### **Conclusion.**  
$X_k \to 0$ in probability.

---

## **(c) Convergence in probability but not almost surely**

### **Claim.**  
Under the assumptions
- independence,  
- $P(|X_k|>1)=1/k$,  
- $P(|X_k|\le 1/\sqrt{k})\to 1$,

we have $X_k \to 0$ in probability but **not** almost surely.

### **Proof.**

### **Step 1: Convergence in probability**

For any fixed $\varepsilon>0$:

- If $\varepsilon \le 1$, then
  $$
  P(|X_k|>\varepsilon)\le P(|X_k|>1)=\frac1k\to 0.
  $$

- If $\varepsilon > 1$, then eventually $|X_k|\le 1/\sqrt{k}<\varepsilon$ with probability $\to 1$, so again  
  $$
  P(|X_k|>\varepsilon)\to0.
  $$

Thus $X_k \to 0$ in probability.

---

### **Step 2: Almost sure convergence fails**

Consider the events
$$
A_k=\{|X_k|>1\}.
$$
We are told
$$
P(A_k)=\frac1k.
$$
Since the $A_k$ are independent, apply the **Second Borel–Cantelli Lemma**:
$$
\sum_{k=1}^{\infty} P(A_k)
= \sum_{k=1}^{\infty}\frac1k
= \infty,
\qquad\Rightarrow\qquad
P(A_k \text{ i.o.})=1.
$$

Thus with probability 1, infinitely many $k$ satisfy $|X_k|>1$, which makes
$$
X_k \not\to 0 \quad a.s.
$$

### **Conclusion.**  
We have  
$$
X_k \to 0 \text{ in probability},  
\qquad
X_k \not\to 0 \text{ a.s.}
$$

---

# **Key Takeaways**

- **Distribution vs. Probability Convergence:**  
  When the limit is a **constant**, convergence in distribution and convergence in probability are equivalent.

- **Monotone Tail Events:**  
  The sets  
  $A_k=\cup_{m=k}^{\infty}\{|X_m|>\varepsilon\}$  
  form a decreasing sequence. Continuity from above is a powerful tool for a.s. arguments.

- **a.s. ⇒ in Probability:**  
  A standard technique: relate single-time deviations to tail deviations, then squeeze using monotonicity.

- **Using Tail Bounds to Show Convergence in Probability:**  
  Split the analysis into “regular part” ($|X_k|\le 1$) and “tail part” ($|X_k|>1$).  
  Evaluate each separately.

- **Borel–Cantelli II:**  
  Independence + $\sum P(A_k)=\infty$ ⇒ events occur infinitely often a.s.  
  This is the standard way to build counterexamples where convergence in probability holds but a.s. fails.

- **General Strategy:**  
  If asked whether $X_k\to0$ a.s., examine  
  $P(|X_k|>\varepsilon)$ for fixed $\varepsilon$,  
  then ask whether large deviations occur infinitely often.

---

If you'd like, I can now generate **2023-Q3**, **Q4**, etc., in the same format.
