---
title: "2025-Q4: Truncated SLLN with General Normalization"
permalink: /probability/prelim/2025q4/
tags: [probability-prelim, strong-law, truncation]
---

## Introduction

This problem is a **generalized Strong Law of Large Numbers** argument using truncation and a nonstandard normalization sequence $a_n$. It tests whether you can:

- Control heavy tails using **Borel–Cantelli** and truncation,
- Prove almost sure convergence via **variance summability**,
- Correctly combine **Kolmogorov’s two-series theorem** with **Kronecker’s lemma**.

**Main tools used**
- Borel–Cantelli Lemma
- Tonelli’s theorem
- Kolmogorov two-series theorem
- Kronecker’s lemma
- Summation by parts (discrete integration by parts)

**Pattern recognition cue**
> When you see truncation $Y_k = X_k \mathbf{1}_{\{|X_k|\le a_k\}}$ and assumptions on  
> $\sum P(|X|\ge a_k)$ and $\sum a_k^{-2}$, think **three-series theorem / truncated SLLN**.

---

## Problem Statement

> **Problem 4.**  
> Let $X$ be a random variable that satisfies
> $$
> \sum_{k=1}^\infty P(|X|\ge a_k) < \infty,
> $$
> where $\{a_k\}_{k\ge1}$ is a non-negative and non-decreasing sequence of real numbers that satisfy  
> (i) $a_k \to \infty$, and  
> (ii) there exists $C<\infty$ so that for each $n\ge1$,
> $$
> \sum_{k=n}^\infty a_k^{-2} \le C n a_n^{-2}.
> $$
>
> Let $\{X_k\}_{k\ge1}$ be an i.i.d. sequence of random variables, and denote
> $$
> Y_k = X_k \mathbf{1}_{\{|X_k|\le a_k\}}, \quad k\ge1.
> $$
>
> **(a)**  
> (i) Prove that
> $$
> \frac{1}{a_n}\sum_{k=1}^n X_k \mathbf{1}_{\{|X_k|>a_k\}} \xrightarrow[n\to\infty]{} 0
> \quad \text{a.s.}
> $$
> (ii) If
> $$
> \frac{1}{a_n}\sum_{k=1}^n (Y_k - E[Y_k]) \xrightarrow[n\to\infty]{} 0
> \quad \text{a.s.},
> $$
> then
> $$
> \frac{1}{a_n}\sum_{k=1}^n (X_k - E[Y_k]) \xrightarrow[n\to\infty]{} 0
> \quad \text{a.s.}
> $$
>
> **(b)**  
> (i) Let $a_0=0$. Prove:
> $$
> \sum_{n=1}^\infty a_n^{-2}E(Y_n^2)
> = \sum_{k=1}^\infty E\!\left(X^2\mathbf{1}_{\{a_{k-1}<|X|\le a_k\}}\right)
> \left(\sum_{n=k}^\infty a_n^{-2}\right).
> $$
> *(Hint: $(0,a_n]=\bigcup_{k=1}^n(a_{k-1},a_k]$).*
>
> (ii) Use the result of part (i) to prove that
> $$
> \sum_{n=1}^\infty a_n^{-2}E(Y_n^2)
> \le C\sum_{k=1}^\infty kP(a_{k-1}<|X|\le a_k).
> $$
>
> **(c)**  
> (i) Prove that
> $$
> \sum_{k=1}^\infty \frac{Y_k - E[Y_k]}{a_k}
> \quad \text{converges a.s.}
> $$
> *(Hint: $\sum_{k=1}^\infty kP(a_{k-1}<|X|\le a_k)
> = \sum_{k=0}^\infty P(|X|>a_k)$ by summation by parts.)*
>
> (ii) Prove that
> $$
> \frac{1}{a_n}\sum_{k=1}^n (X_k - E[Y_k])
> \xrightarrow[n\to\infty]{} 0
> \quad \text{a.s.}
> $$

---

## Solutions

### Part (a)(i) — Tail term vanishes

**Claim.**  
$$
\frac{1}{a_n}\sum_{k=1}^n X_k \mathbf{1}_{\{|X_k|>a_k\}} \to 0 \quad \text{a.s.}
$$

**Proof.**  
Since $\sum_{k=1}^\infty P(|X|\ge a_k)<\infty$, by the **Borel–Cantelli lemma**,
$$
P(|X_k|\ge a_k \ \text{i.o.})=0.
$$
Thus almost surely there exists a (random) $N$ such that for all $k>N$,
$|X_k|<a_k$. Split the sum:
$$
\frac{1}{a_n}\sum_{k=1}^n X_k\mathbf{1}_{\{|X_k|>a_k\}}
= \frac{1}{a_n}\left(
\sum_{k=1}^N X_k\mathbf{1}_{\{|X_k|>a_k\}}
+ \sum_{k=N+1}^n X_k\mathbf{1}_{\{|X_k|>a_k\}}
\right).
$$
The second sum is identically zero for all large $n$. The first sum is finite, hence bounded by some constant $M$. Since $a_n\to\infty$,
$$
\frac{M}{a_n}\to0.
$$

**Conclusion.**  
The normalized tail contribution vanishes almost surely.

**Key Takeaways.**
- **Borel–Cantelli**: finite tail probabilities imply only finitely many large jumps.
- Split sums into “finite bad part + eventually zero tail”.
- Normalization $a_n\to\infty$ kills finite constants.

---

### Part (a)(ii) — Reducing $X_k$ to $Y_k$

**Claim.**  
If
$$
\frac{1}{a_n}\sum_{k=1}^n (Y_k - E[Y_k]) \to 0 \ \text{a.s.},
$$
then
$$
\frac{1}{a_n}\sum_{k=1}^n (X_k - E[Y_k]) \to 0 \ \text{a.s.}
$$

**Proof.**  
Decompose
$$
X_k = Y_k + X_k\mathbf{1}_{\{|X_k|>a_k\}}.
$$
Then
$$
\frac{1}{a_n}\sum_{k=1}^n (X_k - E[Y_k])
= \frac{1}{a_n}\sum_{k=1}^n (Y_k - E[Y_k])
+ \frac{1}{a_n}\sum_{k=1}^n X_k\mathbf{1}_{\{|X_k|>a_k\}}.
$$
The first term converges to zero by assumption, the second by part (a)(i).

**Conclusion.**  
Controlling the truncated centered sum suffices.

**Key Takeaways.**
- Always isolate the tail term explicitly.
- Part (a)(i) is designed to be reused here.

---

### Part (b)(i) — Shell decomposition identity

**Claim.**
$$
\sum_{n=1}^\infty a_n^{-2}E(Y_n^2)
= \sum_{k=1}^\infty E\!\left(X^2\mathbf{1}_{\{a_{k-1}<|X|\le a_k\}}\right)
\left(\sum_{n=k}^\infty a_n^{-2}\right).
$$

**Proof.**  
By definition,
$$
E(Y_n^2)=E\!\left(X^2\mathbf{1}_{\{|X|\le a_n\}}\right).
$$
Since
$$
\{|X|\le a_n\} = \bigcup_{k=1}^n \{a_{k-1}<|X|\le a_k\},
$$
with disjoint union,
$$
E(Y_n^2)=\sum_{k=1}^n E\!\left(X^2\mathbf{1}_{\{a_{k-1}<|X|\le a_k\}}\right).
$$
Thus
$$
\sum_{n=1}^\infty a_n^{-2}E(Y_n^2)
= \sum_{n=1}^\infty a_n^{-2}\sum_{k=1}^n E\!\left(X^2\mathbf{1}_{\{a_{k-1}<|X|\le a_k\}}\right).
$$
All terms are nonnegative, so by **Tonelli** we may swap sums:
$$
= \sum_{k=1}^\infty E\!\left(X^2\mathbf{1}_{\{a_{k-1}<|X|\le a_k\}}\right)
\sum_{n=k}^\infty a_n^{-2}.
$$

**Conclusion.**  
The variance sum decomposes cleanly by shells.

**Key Takeaways.**
- Partition the truncation set into disjoint shells.
- Nonnegativity unlocks Tonelli without hesitation.

---

### Part (b)(ii) — Bounding the variance sum

**Claim.**
$$
\sum_{n=1}^\infty a_n^{-2}E(Y_n^2)
\le C\sum_{k=1}^\infty kP(a_{k-1}<|X|\le a_k).
$$

**Proof.**  
From part (b)(i) and the assumption,
$$
\sum_{n=k}^\infty a_n^{-2} \le Ck a_k^{-2}.
$$
Hence
$$
\sum_{n=1}^\infty a_n^{-2}E(Y_n^2)
\le C\sum_{k=1}^\infty k a_k^{-2}
E\!\left(X^2\mathbf{1}_{\{a_{k-1}<|X|\le a_k\}}\right).
$$
On $\{a_{k-1}<|X|\le a_k\}$, we have $X^2\le a_k^2$, so
$$
E\!\left(X^2\mathbf{1}_{\{a_{k-1}<|X|\le a_k\}}\right)
\le a_k^2 P(a_{k-1}<|X|\le a_k).
$$
Canceling $a_k^2 a_k^{-2}=1$ gives the result.

**Conclusion.**  
The variance sum is controlled by weighted tail probabilities.

**Key Takeaways.**
- Always exploit the bound $X^2\le a_k^2$ on truncation shells.
- This is where the hypothesis on $\sum a_n^{-2}$ is used.

---

### Part (c)(i) — Almost sure convergence of the series

**Claim.**
$$
\sum_{k=1}^\infty \frac{Y_k - E[Y_k]}{a_k}
\quad \text{converges a.s.}
$$

**Proof.**  
Define
$$
Z_k = \frac{Y_k - E[Y_k]}{a_k}.
$$
Then $E[Z_k]=0$ and
$$
\operatorname{Var}(Z_k) = a_k^{-2}E(Y_k^2).
$$
By part (b)(ii),
$$
\sum_{k=1}^\infty \operatorname{Var}(Z_k)
\le C\sum_{k=1}^\infty kP(a_{k-1}<|X|\le a_k).
$$
By summation by parts,
$$
\sum_{k=1}^\infty kP(a_{k-1}<|X|\le a_k)
= \sum_{k=0}^\infty P(|X|>a_k)
< \infty.
$$
Hence $\sum \operatorname{Var}(Z_k)<\infty$, and by the **Kolmogorov two-series theorem**, $\sum Z_k$ converges a.s.

**Conclusion.**  
The normalized truncated centered series converges almost surely.

**Key Takeaways.**
- Variance summability is the gateway to almost sure convergence.
- Summation by parts converts shell probabilities into tail probabilities.

---

### Part (c)(ii) — Final SLLN conclusion

**Claim.**
$$
\frac{1}{a_n}\sum_{k=1}^n (X_k - E[Y_k]) \to 0 \quad \text{a.s.}
$$

**Proof.**  
From part (c)(i), $\sum (Y_k - E[Y_k])/a_k$ converges a.s. Since $a_k$ is increasing and $a_k\to\infty$, **Kronecker’s lemma** implies
$$
\frac{1}{a_n}\sum_{k=1}^n (Y_k - E[Y_k]) \to 0 \quad \text{a.s.}
$$
Part (a)(ii) then transfers this result to $X_k$.

**Conclusion.**  
The normalized centered sum of $X_k$ converges almost surely to zero.

**Key Takeaways.**
- Kronecker’s lemma converts series convergence into normalized partial sums.
- This closes the truncated SLLN loop.

---

## Master Key Takeaways

- Truncate first, normalize later.
- Use Borel–Cantelli to kill rare large jumps.
- Variance summability + Kolmogorov gives a.s. convergence.
- Kronecker’s lemma is the bridge from series to sums.

## Cheat Sheet Entries to Extract

- **Truncated SLLN template:**  
  Define $Y_k=X_k\mathbf{1}_{\{|X_k|\le a_k\}}$, prove tail vanishes, prove $\sum (Y_k-EY_k)/a_k$ converges.
- **Shell decomposition trick:**  
  $\{|X|\le a_n\}=\bigcup_{k\le n}\{a_{k-1}<|X|\le a_k\}$.
- **Summation by parts identity:**  
  $\sum kP(a_{k-1}<|X|\le a_k)=\sum P(|X|>a_k)$.

## Notes on My Original Work

- Strong overall structure, correct theorem selection.
- Borel–Cantelli and Tonelli used exactly where needed.
- Minor notation looseness, but no logical gaps.
- This solution is prelim-ready and reusable as a template.
