Let $x_{1}, x_{2} \ldots \quad$ ind.
(1) Let $\left\{x_{n}\right\}_{n \geq 1}$ ind. if $\sum_{n=1}^{\infty} V\left(x_{n}\right)<\infty$ then $\sum_{n=1}^{\infty}\left(x_{n}-E\left(x_{n}\right)\right)$ conv. as.
(2) Kol. 3 -series theorem.

$$
\begin{aligned}
\sum_{n=1}^{\infty} x_{n} \text { con as. } \Leftrightarrow & (1) \forall A>0 \sum_{n=1}^{\infty} P\left(\left|X_{n}\right|>A\right)<\infty \\
& \text { (2) } \sum_{n=1}^{\infty} \operatorname{Var}\left(Y_{n}\right)<\infty \\
& \text { (3) } \sum_{n=1}^{n} E\left(Y_{n}\right) \text { conv. as. }
\end{aligned}
$$

(3) Lemma that follow H-S in Equility. $\quad S_{n}=\sum x_{n}$.
if $\operatorname{Sup}_{n \geq 1}\left\{\left|S_{n}\right|\right\}<\infty$, as. and $E\left(\operatorname{Sup}_{n \geq 1}\left\{x_{n}^{2}\right\}\right]<\infty$.
then. $\sum_{n=1}^{\infty} \operatorname{Var}\left(x_{n}\right)<\infty$
$\stackrel{(1)}{\Rightarrow} \sum\left(x_{n}-E\left(x_{n}\right)\right)$ conv. a.s.
it Follows from 3 : $\left\{x_{n}\right\}_{n \geq 1} \ln D, E\left(X_{n}\right)=0, n \geq 1$
$\sup _{n \geq 1}\left|S_{n}\right|<\infty,\left|x_{n}\right| \leqslant A, \quad n \geq 1$
then $\sum_{n=1}^{\infty} x_{n}$ conv. As.
if Pantial sum is Buanded thus Thun converse,
$\lim _{n \rightarrow \infty} S_{n}$ conv. as.
used To prove ⇒ on Kol. 3-series.

## Mroseckor Lemma.

Let $x_{n} \in \mathbb{R}, n>1,0<a_{n} \uparrow \infty$. if $\sum_{n=1}^{\infty} \frac{x_{n}}{a_{n}}$ converges.
then, $\quad \frac{\sum_{k=1}^{n} x_{k}}{a_{n}}=\frac{s_{n}}{a_{n}} \xrightarrow[n \rightarrow \infty]{ } 0$
Recall: $G, F:(a, b] \rightarrow \mathbb{R}$.

$$
\begin{aligned}
& \int_{c a, b]} F(x) \partial G(x)+\int_{(a, b]} G(x) \partial F(x)=[F \cdot G]_{a}^{b}+\sum_{a<x<b} \Delta F(x) \Delta G(x) \\
& \text { of The Jup, } \\
& a_{0}=0=b_{0} \\
& b_{n}=\sum \frac{x_{k}}{a_{k}}, n \geq 1
\end{aligned}
$$

Lebesgue stebes.
$F(b) G(b)-F(a) G(d)$

$$
\begin{aligned}
& a_{0}-0=b_{0} \\
& b_{n}=\sum \frac{x_{k}}{a_{k}}, n \geq 1 \\
& \text { ムいさん、い } \\
& b_{n}=\sum \frac{x_{k}}{a_{k}}, n \geq 1 \\
& \underbrace{\sum_{i=1}^{n} b_{i}\left(a_{i}-a_{i-1}\right)}_{\substack{i=1 \\
\text { Minous thit. }}}+\underbrace{\sum_{n=1}^{n} b_{r}=b_{\infty}} \\
& =b_{n} a_{n}+\sum_{n=1}^{n}\left(a_{i}-a_{i-1}\right)\left(b_{i}-b_{i-1}\right) \\
& \sum_{i=1}^{n} \frac{a_{i}\left(b_{i}-b_{i-1}\right)}{a_{n}}=b_{n}-\sum_{i=1}^{n} b_{i-1}\left(\frac{a_{i}-a_{i-1}}{a_{n}}\right) \xrightarrow[n \rightarrow \infty]{ } b_{00}-b_{00 .}=0 . \\
& b_{i}-b_{i-1}=\frac{x_{i}}{a!} \quad \sum_{i=1}^{n} \frac{a_{i}\left(b_{i}-b_{i-1}\right)}{a_{n}} \\
& \sum_{i=1}^{n} \frac{x_{i}}{a_{n}} \xrightarrow[n \rightarrow \infty]{ } 0
\end{aligned}
$$

right continuous

Theonem
$\left\{x_{n}\right\} I_{n} D, E\left(x_{n}\right)=0, n \geq 1$ ，Assume $a_{n} \uparrow \infty, \sum_{n=1}^{\infty} \frac{E\left(x_{n}^{2}\right)}{a_{n}^{2}}<\infty$
then $\frac{S_{n}}{a_{n}} \xrightarrow[n \rightarrow \infty]{\text { as．}} 0$
$\Rightarrow \sum_{n=1}^{\infty} \frac{x_{n}}{a_{n}}$ conv a．s．

Ex $\left\{x_{n}\right\}_{n \geq 1} E\left(x_{n}\right)=0, \quad \operatorname{Sup}_{n}\left\{E\left(x_{n}^{2}\right)\right\}<c<\infty$
$\Rightarrow \frac{\sum_{k=1}^{n} x_{k}}{a_{n}} \xrightarrow[n \rightarrow \infty]{\text { a．s．}} 0$

Excotly krovector сегиа．
if second momeat $\varepsilon_{\text {Eists }}$ then．First，rant $9 x^{2}+s t ~ \epsilon_{1} \mid \leq \epsilon^{\prime h}\left(x^{2}\right)$ By holder．

$$
\text { sups } L^{\prime} \leq C^{2}
$$

Since

$$
\begin{aligned}
\sum_{n=1}^{\infty} E\left[\frac{x_{n}^{2}}{n[\log n]^{1+\varepsilon}}\right] & \leq c \sum \frac{1}{n[\log (n)]^{1+\varepsilon}}<\infty \quad \varepsilon>0 \\
& \int_{x=2}^{\infty} \frac{\partial x}{x \log (x)} 1+\epsilon \int_{y=\log (2)}^{\infty} \frac{1}{y^{1+n}} d y<\infty
\end{aligned}
$$

then $\frac{S_{n}}{\sqrt{n}[\operatorname{Cog}(n)]^{1 / L+\epsilon}} \xrightarrow[n \rightarrow \infty]{a_{1} s .} 0$.
↑

## comes fran $E\left[x_{i}^{2}\right]$. eg. $n^{1 / 2}$.

if we Assume iid.
SLLN of Marcinkiewic $z-z y$ gmond
$X_{1}, X_{2} \ldots$ ind $\quad E\left(X_{i}\right)=0 \quad E|X|^{p}<\infty \quad 1 \leqslant p<Z$
then $\frac{\delta}{n}_{n+p} \xrightarrow[n \rightarrow 00]{\text { a.s. }} 0$.
Proof $Y_{k}=X_{k} \cdot \mathbb{1}_{\left\{\left|x_{k}\right| \leq k \mid P\right\}}, \quad T_{n}=\sum_{k=1}^{n} Y_{k}$.
$\sum_{k=1}^{\infty} P\left(x_{k} \neq y_{k}\right)=\sum_{k=1}^{\infty} P\left(|x|^{p}>k\right) \sim E\left(\left.x\right|^{p}<\infty\right.$.
By BCI. $\quad P\left(X_{k} \neq Y_{k} \quad i .0.\right)=0$
Enourch to show.
ETS $\frac{T_{n}}{n^{1 / p}} \xrightarrow[n \rightarrow \infty]{\text { as. }} 0$
Also: $\sum \operatorname{Var}\left(\frac{k_{k}}{k 1 / p}\right) \quad$ use leavan with good sequenters

